import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.stats import spearmanr
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers, callbacks, Input
from tensorflow.keras.models import Model
import warnings

warnings.filterwarnings('ignore')
plt.style.use('ggplot')

def check_gpu_availability():
    """æª¢æŸ¥ GPU ç‹€æ…‹"""
    print("\n" + "="*60)
    print("ğŸ” ç¡¬é«”è¨­å‚™æª¢æ¸¬")
    print("="*60)
    
    # TensorFlow ç‰ˆæœ¬
    print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
    
    # æª¢æŸ¥å¯ç”¨çš„ç‰©ç†è¨­å‚™
    physical_devices = tf.config.list_physical_devices()
    print(f"\næ‰€æœ‰å¯ç”¨è¨­å‚™:")
    for device in physical_devices:
        print(f"  - {device.device_type}: {device.name}")
    
    # æª¢æŸ¥ GPU
    gpus = tf.config.list_physical_devices('GPU')
    gpu_available = len(gpus) > 0
    
    print(f"\nğŸ–¥ï¸  GPU è¨­å‚™:")
    if gpu_available:
        print(f"  âœ… æ‰¾åˆ° {len(gpus)} å€‹ GPU: {[gpu.name for gpu in gpus]}")
        
        # å°æ–¼ Apple Siliconï¼Œé¡¯ç¤ºé¡å¤–è³‡è¨Š
        import platform
        if platform.processor() == 'arm':
            print(f"\n  ğŸ Apple Silicon åµæ¸¬:")
            print(f"     è™•ç†å™¨: {platform.processor()}")
            print(f"     ç³»çµ±: {platform.system()} {platform.release()}")
            print(f"     âœ… ä½¿ç”¨ Metal å¾Œç«¯é€²è¡Œ GPU åŠ é€Ÿ")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")
    
    print("="*60 + "\n")
    
    return len(gpus) > 0

def split_features_by_type(X, feature_names):
    """
    â­ é—œéµå‡½æ•¸ï¼šå°‡ç‰¹å¾µå€åˆ†ç‚ºã€ŒåŸºå› ã€èˆ‡ã€Œè—¥ç‰©ã€
    
    é‚è¼¯ï¼š
    GDSC è³‡æ–™é›†ä¸­ï¼ŒåŸºå› é€šå¸¸æ˜¯å¤§å¯«å­—æ¯ (å¦‚ BRAF, TP53)ï¼Œ
    è€Œè—¥ç‰©ç‰¹å¾µé€šå¸¸åŒ…å«å°å¯«ã€æ•¸å­—æˆ–ç‰¹å®šé—œéµå­— (å¦‚ PubChem, drug, descriptors)ã€‚
    """
    feature_names = list(feature_names)
    
    # å®šç¾©è—¥ç‰©ç‰¹å¾µçš„é—œéµå­— (æ ¹æ“šä½ çš„è³‡æ–™é›†èª¿æ•´)
    # å¦‚æœä½ çš„è—¥ç‰©ç‰¹å¾µæ˜¯ One-Hot (å¦‚ 'DRUG_Name'), æˆ–è€…æ˜¯æŒ‡ç´‹ (Fingerprint)
    drug_keywords = ['drug', 'Drug', 'PubChem', 'fingerprint', 'descriptor']
    
    drug_indices = []
    gene_indices = []
    
    for i, col in enumerate(feature_names):
        # åˆ¤æ–·é‚è¼¯ï¼šå¦‚æœæ¬„ä½åç¨±åŒ…å«è—¥ç‰©é—œéµå­—ï¼Œæˆ–æ˜¯çœ‹èµ·ä¾†ä¸åƒåŸºå›  (åŸºå› é€šå¸¸å…¨æ˜¯è‹±æ–‡å¤§å¯«)
        is_drug = any(k in col for k in drug_keywords)
        
        # å‚™ç”¨é‚è¼¯ï¼šå¦‚æœæ²’æœ‰æ˜ç¢ºé—œéµå­—ï¼Œé€šå¸¸éåŸºå› æ¬„ä½æ¯”è¼ƒå°‘ï¼Œå¯ä»¥ç”¨æ’é™¤æ³•
        # å‡è¨­åŸºå› æ¬„ä½å¤§å¯«æ¯”ä¾‹å¾ˆé«˜
        if is_drug:
            drug_indices.append(i)
        else:
            gene_indices.append(i)
            
    # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•— (ä¾‹å¦‚å…¨éƒ¨éƒ½è¢«æ­¸é¡ç‚ºåŸºå› )ï¼Œå‰‡å¼·åˆ¶ä½¿ç”¨ç°¡å–®åˆ†å‰²
    # é€™è£¡å‡è¨­å¾Œé¢çš„æ¬„ä½é€šå¸¸æ˜¯è—¥ç‰© (è‹¥è³‡æ–™æœ‰ç¶“éç‰¹å®šæ’åº)
    if len(drug_indices) == 0:
        print("âš ï¸ è­¦å‘Šï¼šç„¡æ³•è‡ªå‹•åµæ¸¬è—¥ç‰©æ¬„ä½ï¼Œå°‡å˜—è©¦ä½¿ç”¨å•Ÿç™¼å¼åˆ†å‰²...")
        # å‡è¨­ç‰¹å¾µæ•¸é‡å°‘æ–¼ 5000 çš„é¡åˆ¥å¯èƒ½æ˜¯è—¥ç‰©ï¼Œæˆ–è€…ç›´æ¥å–å¾Œ 20%
        # é€™è£¡åƒ…ä½œç¤ºç¯„ï¼Œå»ºè­°ä½¿ç”¨è€…ç¢ºèªæ¬„ä½åç¨±
        split_point = int(len(feature_names) * 0.9) 
        gene_indices = list(range(split_point))
        drug_indices = list(range(split_point, len(feature_names)))

    print(f"  ç‰¹å¾µåˆ†é›¢çµæœ: åŸºå› ç‰¹å¾µ {len(gene_indices)} å€‹, è—¥ç‰©ç‰¹å¾µ {len(drug_indices)} å€‹")
    
    # è½‰æ›ç‚º Numpy ä¸¦åˆ†å‰²
    X_genes = X[:, gene_indices]
    X_drugs = X[:, drug_indices]
    
    return X_genes, X_drugs, gene_indices, drug_indices

"""
def build_dual_branch_model(gene_dim, drug_dim, learning_rate=0.0001):
    
    # â­ å»ºç«‹é›™åˆ†æ”¯æ¨¡å‹ (Dual-Branch Architecture)
    
    # Branch 1: è™•ç†åŸºå›  (Gene Expression)
    # Branch 2: è™•ç†è—¥ç‰© (Drug Descriptors/Fingerprints)
    # Fusion:   çµåˆå…©è€…é€²è¡Œé æ¸¬
    
    
    # --- Branch 1: Gene Tower (åŸºå› å¡”) ---
    gene_input = Input(shape=(gene_dim,), name='gene_input')
    
    # ä½¿ç”¨æ·±å±¤ç¶²è·¯å£“ç¸®åŸºå› è³‡è¨Š (é¡ä¼¼ Autoencoder çš„ Encoder éƒ¨åˆ†)
    x = layers.Dense(1024, kernel_regularizer=regularizers.l2(1e-4))(gene_input)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.3)(x)
    
    x = layers.Dense(512, kernel_regularizer=regularizers.l2(1e-4))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.3)(x)
    
    gene_features = layers.Dense(256, activation='relu', name='gene_latent')(x)
    
    # --- Branch 2: Drug Tower (è—¥ç‰©å¡”) ---
    drug_input = Input(shape=(drug_dim,), name='drug_input')
    
    y = layers.Dense(512, kernel_regularizer=regularizers.l2(1e-4))(drug_input)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)
    y = layers.Dropout(0.2)(y)
    
    y = layers.Dense(256, kernel_regularizer=regularizers.l2(1e-4))(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)
    
    drug_features = layers.Dense(128, activation='relu', name='drug_latent')(y)
    
    # --- Fusion Layer (èåˆå±¤) ---
    # å°‡åŸºå› ç‰¹å¾µèˆ‡è—¥ç‰©ç‰¹å¾µæ‹¼æ¥
    combined = layers.Concatenate()([gene_features, drug_features])
    
    # --- Prediction Head (é æ¸¬å±¤) ---
    z = layers.Dense(512, activation='relu')(combined)
    z = layers.Dropout(0.3)(z)
    
    z = layers.Dense(256, activation='relu')(z)
    z = layers.Dropout(0.2)(z)
    
    z = layers.Dense(64, activation='relu')(z)
    
    # è¼¸å‡ºå±¤ (å›æ­¸é æ¸¬ IC50)
    output = layers.Dense(1, activation='linear', name='output')(z)
    
    model = Model(inputs=[gene_input, drug_input], outputs=output, name='Dual_Branch_Network')
    
    # ç·¨è­¯æ¨¡å‹
    optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mse'])
    
    return model
"""

def build_dual_branch_model(gene_dim, drug_dim, learning_rate=0.000001):  # â­â­ æ¥µä½å­¸ç¿’ç‡é˜²æ­¢ Metal çˆ†ç‚¸
    """
    â­ ç©©å®šç‰ˆï¼šå°ˆç‚º Mac Metal å„ªåŒ–ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    
    ä¿®æ”¹ç­–ç•¥ï¼š
    1. ç§»é™¤æ‰€æœ‰ BatchNormalizationï¼ˆMetal ä¸Šæ•¸å€¼ä¸ç©©å®šï¼‰
    2. ä½¿ç”¨ He initialization åˆå§‹åŒ–æ¬Šé‡
    3. æ¥µå¼·æ¢¯åº¦è£å‰ª (clipnorm=0.5)
    4. é™ä½ L2 æ­£å‰‡åŒ–å¼·åº¦
    5. ä½¿ç”¨æ¥µä½çš„å­¸ç¿’ç‡
    """
    
    # He initialization for ReLU activations
    initializer = keras.initializers.HeNormal(seed=42)
    
    # --- Branch 1: Gene Tower (ç°¡åŒ–ç‰ˆ) ---
    gene_input = Input(shape=(gene_dim,), name='gene_input')
    
    x = layers.Dense(512, 
                     kernel_initializer=initializer,
                     kernel_regularizer=regularizers.l2(1e-5),  # â­ é™ä½æ­£å‰‡åŒ–
                     activation='relu')(gene_input)
    x = layers.Dropout(0.4)(x)  # â­ å¢åŠ  dropout æ›¿ä»£ BN
    
    x = layers.Dense(256, 
                     kernel_initializer=initializer,
                     kernel_regularizer=regularizers.l2(1e-5),
                     activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    
    gene_features = layers.Dense(128, 
                                 kernel_initializer=initializer,
                                 activation='relu', 
                                 name='gene_latent')(x)
    
    # --- Branch 2: Drug Tower (ç°¡åŒ–ç‰ˆ) ---
    drug_input = Input(shape=(drug_dim,), name='drug_input')
    
    y = layers.Dense(256, 
                     kernel_initializer=initializer,
                     kernel_regularizer=regularizers.l2(1e-5),
                     activation='relu')(drug_input)
    y = layers.Dropout(0.3)(y)
    
    y = layers.Dense(128, 
                     kernel_initializer=initializer,
                     kernel_regularizer=regularizers.l2(1e-5),
                     activation='relu')(y)
    
    drug_features = layers.Dense(64, 
                                 kernel_initializer=initializer,
                                 activation='relu', 
                                 name='drug_latent')(y)
    
    # --- Fusion Layer ---
    combined = layers.Concatenate()([gene_features, drug_features])
    
    # --- Prediction Head (ç°¡åŒ–ç‰ˆ) ---
    z = layers.Dense(128, 
                     kernel_initializer=initializer,
                     activation='relu')(combined)
    z = layers.Dropout(0.3)(z)
    
    z = layers.Dense(64, 
                     kernel_initializer=initializer,
                     activation='relu')(z)
    z = layers.Dropout(0.2)(z)
    
    # è¼¸å‡ºå±¤
    output = layers.Dense(1, 
                          kernel_initializer=initializer,
                          activation='linear', 
                          name='output')(z)
    
    model = Model(inputs=[gene_input, drug_input], outputs=output, name='Dual_Branch_Network_Stable')
    
    # â­â­â­ é—œéµä¿®æ­£ï¼šæ¥µå¼·çš„æ¢¯åº¦è£å‰ª + æ¥µä½å­¸ç¿’ç‡
    optimizer = keras.optimizers.legacy.Adam(
        learning_rate=learning_rate,  # 0.000001
        clipnorm=0.5,      # â­ å¼·åŠ›æ¢¯åº¦è£å‰ª
        clipvalue=0.5,     # â­ é›™é‡ä¿éšªï¼šåŒæ™‚é™åˆ¶æ¢¯åº¦çµ•å°å€¼
        epsilon=1e-7       # â­ æ•¸å€¼ç©©å®šæ€§
    )
    
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mse'])
    
    return model

def train_dual_model(X_train_g, X_train_d, y_train, X_val_g, X_val_d, y_val, epochs=150, batch_size=256):  # â­ å¢å¤§ batch_size
    """è¨“ç·´é›™åˆ†æ”¯æ¨¡å‹ - ç©©å®šç‰ˆ"""
    print("\n" + "="*50)
    print("ğŸš€ é–‹å§‹è¨“ç·´é›™åˆ†æ”¯æ·±åº¦å­¸ç¿’æ¨¡å‹ (Dual-Branch DL)")
    print("="*50)
    
    gene_dim = X_train_g.shape[1]
    drug_dim = X_train_d.shape[1]
    
    print(f"  Gene features: {gene_dim}")
    print(f"  Drug features: {drug_dim}")
    print(f"  Training samples: {len(y_train)}")
    print(f"  Validation samples: {len(y_val)}")
    
    model = build_dual_branch_model(gene_dim, drug_dim)
    model.summary()
    
    # Callbacks - æ›´ä¿å®ˆçš„è¨­å®š
    early_stop = callbacks.EarlyStopping(
        monitor='val_loss', 
        patience=30,  # â­ å¢åŠ è€å¿ƒï¼Œå› ç‚ºå­¸ç¿’ç‡å¾ˆä½
        restore_best_weights=True, 
        verbose=1
    )
    
    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss', 
        factor=0.5, 
        patience=12,  # â­ å¢åŠ è€å¿ƒ
        min_lr=1e-8,  # â­ å…è¨±æ›´ä½çš„å­¸ç¿’ç‡
        verbose=1
    )
    
    # â­ æ–°å¢ï¼šNaN æª¢æ¸¬ï¼Œå¦‚æœ loss è®Šæˆ NaN ç«‹å³åœæ­¢
    terminate_on_nan = callbacks.TerminateOnNaN()
    
    history = model.fit(
        x=[X_train_g, X_train_d],
        y=y_train,
        validation_data=([X_val_g, X_val_d], y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop, reduce_lr, terminate_on_nan],
        verbose=1
    )
    
    return model, history

def evaluate_model(y_true, y_pred, model_name="Dual-Branch DL"):
    """è©•ä¼°æ¨¡å‹æ•ˆèƒ½"""
    y_pred = y_pred.flatten()
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    
    # Spearman ç›¸é—œä¿‚æ•¸
    spearman_corr, spearman_pval = spearmanr(y_true, y_pred)
    
    print("\n" + "="*50)
    print(f"{model_name} è©•ä¼°çµæœ")
    print("="*50)
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  RMSE:     {rmse:.4f}")
    print(f"  MAE:      {mae:.4f}")
    print(f"  Spearman Correlation: {spearman_corr:.4f} (p={spearman_pval:.4e})")
    print("="*50)
    
    return {
        'R2': r2, 
        'RMSE': rmse, 
        'MAE': mae,
        'MSE': mse,
        'Spearman_Correlation': spearman_corr,
        'Spearman_PValue': spearman_pval
    }

def plot_results(history, y_test, y_pred, metrics):
    """ç¹ªè£½ Loss æ›²ç·šèˆ‡é æ¸¬æ•£é»åœ–"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Loss Curve
    axes[0].plot(history.history['loss'], label='Train Loss')
    axes[0].plot(history.history['val_loss'], label='Val Loss')
    axes[0].set_title('Learning Curve (Loss)')
    axes[0].set_xlabel('Epochs')
    axes[0].set_ylabel('MSE')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Prediction Scatter
    y_pred_flat = y_pred.flatten()
    axes[1].scatter(y_test, y_pred_flat, alpha=0.5, s=20)
    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    axes[1].set_title(f'Actual vs Predicted (RÂ²={metrics["R2"]:.3f})')
    axes[1].set_xlabel('Actual IC50')
    axes[1].set_ylabel('Predicted IC50')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('dl_dual_branch_results.png')
    print("âœ“ çµæœåœ–è¡¨å·²å„²å­˜è‡³ dl_dual_branch_results.png")
    plt.close()
"""
def run_deep_learning_pipeline(X_train, X_test, y_train, y_test, feature_names):
    
    # åŸ·è¡Œå®Œæ•´çš„é›™åˆ†æ”¯æ·±åº¦å­¸ç¿’æµç¨‹
    
    check_gpu_availability()
    
    # 1. ç¢ºä¿è³‡æ–™æ ¼å¼æ­£ç¢º
    if hasattr(X_train, 'values'): 
        X_train = X_train.values
    if hasattr(X_test, 'values'): 
        X_test = X_test.values
    if hasattr(y_train, 'values'): 
        y_train = y_train.values
    if hasattr(y_test, 'values'): 
        y_test = y_test.values
    
    # 2. ç‰¹å¾µæ¨™æº–åŒ– (Standardization) - å°ç¥ç¶“ç¶²è·¯æ¥µç‚ºé‡è¦ï¼
    print("\né€²è¡Œç‰¹å¾µæ¨™æº–åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 3. åˆ†é›¢åŸºå› èˆ‡è—¥ç‰©ç‰¹å¾µ
    print("\næ­£åœ¨åˆ†é›¢åŸºå› èˆ‡è—¥ç‰©ç‰¹å¾µ...")
    X_train_g, X_train_d, g_idx, d_idx = split_features_by_type(X_train_scaled, feature_names)
    X_test_g, X_test_d, _, _ = split_features_by_type(X_test_scaled, feature_names)
    
    # 4. é©—è­‰é›†åˆ‡åˆ†
    X_tr_g, X_val_g, X_tr_d, X_val_d, y_tr, y_val = train_test_split(
        X_train_g, X_train_d, y_train, test_size=0.15, random_state=42
    )
    
    # 5. è¨“ç·´æ¨¡å‹
    model, history = train_dual_model(
        X_tr_g, X_tr_d, y_tr, 
        X_val_g, X_val_d, y_val, 
        epochs=150, batch_size=128
    )
    
    # 6. é æ¸¬èˆ‡è©•ä¼°
    y_pred = model.predict([X_test_g, X_test_d])
    metrics = evaluate_model(y_test, y_pred)
    
    # 7. ç¹ªåœ–
    plot_results(history, y_test, y_pred, metrics)
    
    # ç‚ºäº†ä¿æŒèˆ‡ main.py æ¥å£ä¸€è‡´ï¼Œè¿”å›éƒ¨åˆ†ç‰©ä»¶
    return None, model, y_pred, metrics
"""

def run_deep_learning_pipeline(X_train, X_test, y_train, y_test, feature_names):
    """åŸ·è¡Œå®Œæ•´çš„é›™åˆ†æ”¯æ·±åº¦å­¸ç¿’æµç¨‹ - ç©©å®šç‰ˆ"""
    check_gpu_availability()
    
    # â­ ä¿®æ­£ 1: å¼·åˆ¶è½‰å‹ float32 é˜²æ­¢ Mac Metal æ•¸å€¼æº¢å‡º
    print("\n[System] Converting data to float32...")
    if hasattr(X_train, 'values'): X_train = X_train.values
    if hasattr(X_test, 'values'): X_test = X_test.values
    if hasattr(y_train, 'values'): y_train = y_train.values
    if hasattr(y_test, 'values'): y_test = y_test.values
    
    X_train = np.asarray(X_train).astype('float32')
    X_test = np.asarray(X_test).astype('float32')
    y_train = np.asarray(y_train).astype('float32')
    y_test = np.asarray(y_test).astype('float32')
    
    # â­ ä¿®æ­£ 2: æª¢æŸ¥ NaN å’Œ Inf
    print("\n[Validation] Checking for NaN and Inf values...")
    def check_data(data, name):
        if np.any(np.isnan(data)):
            print(f"  âš ï¸  WARNING: {name} contains NaN values!")
            return False
        if np.any(np.isinf(data)):
            print(f"  âš ï¸  WARNING: {name} contains Inf values!")
            return False
        print(f"  âœ… {name} is clean")
        return True
    
    check_data(X_train, "X_train")
    check_data(X_test, "X_test")
    check_data(y_train, "y_train")
    check_data(y_test, "y_test")
    
    # 2. ç‰¹å¾µæ¨™æº–åŒ–
    print("\n[Preprocessing] Standardizing features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # â­ ä¿®æ­£ 3: é™åˆ¶æ¨™æº–åŒ–å¾Œçš„æ¥µç«¯å€¼ï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
    print("\n[Safety] Clipping extreme values after standardization...")
    X_train_scaled = np.clip(X_train_scaled, -10, 10)  # é™åˆ¶åœ¨ Â±10 æ¨™æº–å·®
    X_test_scaled = np.clip(X_test_scaled, -10, 10)
    
    print(f"  Data range after clipping: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
    
    # 3. åˆ†é›¢ç‰¹å¾µ
    print("\n[Preprocessing] Splitting Gene and Drug features...")
    X_train_g, X_train_d, g_idx, d_idx = split_features_by_type(X_train_scaled, feature_names)
    X_test_g, X_test_d, _, _ = split_features_by_type(X_test_scaled, feature_names)
    
    # 4. é©—è­‰é›†åˆ‡åˆ†
    print("\n[Splitting] Creating validation set...")
    X_tr_g, X_val_g, X_tr_d, X_val_d, y_tr, y_val = train_test_split(
        X_train_g, X_train_d, y_train, test_size=0.15, random_state=42
    )
    
    print(f"  Training set size: {len(y_tr)}")
    print(f"  Validation set size: {len(y_val)}")
    print(f"  Test set size: {len(y_test)}")
    
    # 5. è¨“ç·´æ¨¡å‹
    print("\n" + "="*60)
    print("é–‹å§‹è¨“ç·´æ¨¡å‹ï¼ˆé è¨ˆéœ€è¦æ•¸åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…...ï¼‰")
    print("="*60)
    
    model, history = train_dual_model(
        X_tr_g, X_tr_d, y_tr, 
        X_val_g, X_val_d, y_val, 
        epochs=150, batch_size=256  # â­ ä½¿ç”¨è¼ƒå¤§çš„ batch size
    )
    
    # 6. é æ¸¬èˆ‡è©•ä¼°
    print("\n[Evaluation] Making predictions on test set...")
    y_pred = model.predict([X_test_g, X_test_d], verbose=0)
    metrics = evaluate_model(y_test, y_pred)
    
    # 7. ç¹ªåœ–
    print("\n[Visualization] Creating plots...")
    plot_results(history, y_test, y_pred, metrics)
    
    print("\n" + "="*60)
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print("="*60)
    
    return None, model, y_pred, metrics

# ==================== ä¸»ç¨‹å¼åŸ·è¡Œå€ ====================
if __name__ == "__main__":
    """
    ç¨ç«‹åŸ·è¡Œæ­¤æª”æ¡ˆé€²è¡Œæ¸¬è©¦
    """
    print("æ·±åº¦å­¸ç¿’æ¨¡å‹ (é›™åˆ†æ”¯æ¶æ§‹) - ç¨ç«‹åŸ·è¡Œæ¨¡å¼")
    print("="*60)
    
    # è¼‰å…¥èˆ‡å‰è™•ç†è³‡æ–™
    from main import preprocess_data
    
    file_path = 'Preprocessing/Data_imputed.csv'
    
    try:
        # è³‡æ–™è™•ç†
        X_train, X_test, y_train, y_test, features = preprocess_data(file_path)
        
        # åŸ·è¡Œæ·±åº¦å­¸ç¿’ Pipeline
        encoder, model, y_pred, metrics = run_deep_learning_pipeline(
            X_train, X_test, y_train, y_test, features
        )
        
        print("\næ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°å®Œæˆï¼")
        print("ç”Ÿæˆçš„æª”æ¡ˆ:")
        print("  - dl_dual_branch_results.png")
        
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

# result
# ==================================================
# Dual-Branch DL è©•ä¼°çµæœ
# ==================================================
#   RÂ² Score: 0.4079
#   RMSE:     2.1253
#   MAE:      1.6857
#   Spearman Correlation: 0.5992 (p=0.0000e+00)
# ==================================================