import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler  # â­ æ–°å¢ï¼šç”¨æ–¼ç‰¹å¾µæ¨™æº–åŒ–

# å°å…¥æ·±åº¦å­¸ç¿’æ¨¡å‹
from DL.deep_learning_model import preprocess_data_standalone
from DL.deep_learning_model import run_deep_learning_pipeline

# è¨­å®šç¹ªåœ–é¢¨æ ¼
plt.style.use('ggplot')

def preprocess_data(file_path):
    """
    éšæ®µ 1: è³‡æ–™å‰è™•ç†
    è² è²¬è®€å–ã€æ¸…æ´—ã€ç‰¹å¾µå·¥ç¨‹èˆ‡åˆ†å‰²
    """
    print(f"Step 1: æ­£åœ¨è®€å–è³‡æ–™ä¸¦é€²è¡Œå‰è™•ç†: {file_path}...")

    df = pd.read_csv(file_path)
    target = 'LN_IC50'

    # ç§»é™¤ ID èˆ‡ Data Leakage æ¬„ä½
    drop_cols = [
        'COSMIC_ID', 'CELL_LINE_NAME', 'DRUG_ID',
        'AUC', 'Z_SCORE', 'RMSE',
        'NLME_RESULT_ID', 'NLME_CURVE_ID', 'SANGER_MODEL_ID'
    ]

    existing_drop_cols = [col for col in drop_cols if col in df.columns]
    df_cleaned = df.drop(columns=existing_drop_cols)
    print(f"   -> å·²ç§»é™¤æ¬„ä½: {existing_drop_cols}")

    # One-Hot Encoding
    categorical_cols = df_cleaned.select_dtypes(include=['object']).columns
    df_processed = pd.get_dummies(df_cleaned, columns=categorical_cols, drop_first=True)

    # åˆ†å‰² X, y
    X = df_processed.drop(columns=[target])
    y = df_processed[target]
    feature_names = X.columns

    # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  
    print(f"   -> è³‡æ–™å½¢ç‹€: X_train={X_train.shape}, X_test={X_test.shape}")

    return X_train, X_test, y_train, y_test, feature_names

def train_rf_model(X_train, y_train, X_test):
    """
    éšæ®µ 2-A: è¨“ç·´ Random Forest æ¨¡å‹
    ç¨ç«‹çš„ RF è¨“ç·´é‚è¼¯
    """
    print("\nStep 2-A: æ­£åœ¨è¨“ç·´ Random Forest...")
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # é€²è¡Œé æ¸¬
    y_pred = model.predict(X_test)
    return model, y_pred

def train_xgb_model(X_train, y_train, X_test):
    """
    éšæ®µ 2-B: è¨“ç·´ XGBoost æ¨¡å‹
    ç¨ç«‹çš„ XGBoost è¨“ç·´é‚è¼¯
    """
    print("Step 2-B: æ­£åœ¨è¨“ç·´ XGBoost...")
    model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)

    # é€²è¡Œé æ¸¬
    y_pred = model.predict(X_test)
    return model, y_pred

def evaluate_and_plot_comparison(y_test, rf_pred, xgb_pred, xgb_model, feature_names):
    """
    éšæ®µ 3: è©•ä¼°èˆ‡è¦–è¦ºåŒ–æ¯”è¼ƒ
    æ¥æ”¶å…©å€‹æ¨¡å‹çš„é æ¸¬çµæœé€²è¡Œçµ±ä¸€è©•ä¼°
    """
    print("\nStep 3: æ¯”è¼ƒæ¨¡å‹çµæœ...")

    # --- å…§éƒ¨è©•ä¼°å‡½å¼ ---
    def calculate_metrics(y_true, y_pred, name):
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        print(f"   [{name}] RMSE: {rmse:.4f}, R2: {r2:.4f}")
        return rmse, r2

    print("\n" + "="*30)
    print("æ¨¡å‹æ•ˆèƒ½æŒ‡æ¨™")
    print("="*30)
    rf_rmse, rf_r2 = calculate_metrics(y_test, rf_pred, "Random Forest")
    xgb_rmse, xgb_r2 = calculate_metrics(y_test, xgb_pred, "XGBoost")

    # --- è¦–è¦ºåŒ– ---
    plt.figure(figsize=(14, 6))

    # å·¦åœ–: RF
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, rf_pred, alpha=0.5, color='blue')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.title(f'Random Forest\nRMSE: {rf_rmse:.3f}')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')

    # å³åœ–: XGB
    plt.subplot(1, 2, 2)
    plt.scatter(y_test, xgb_pred, alpha=0.5, color='green')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.title(f'XGBoost\nRMSE: {xgb_rmse:.3f}')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')

    plt.tight_layout()
    plt.savefig('model_comparison.png')
    print("\nåœ–è¡¨å·²ä¿å­˜ç‚º 'model_comparison.png'")

    # --- ç‰¹å¾µé‡è¦æ€§ (XGB) ---
    feat_imp = pd.DataFrame({
        'feature': feature_names,
        'importance': xgb_model.feature_importances_
    }).sort_values('importance', ascending=False).head(10)

    print("\n" + "="*30)
    print("XGBoost å‰ 10 é‡è¦ç‰¹å¾µ")
    print("="*30)
    print(feat_imp)


# --- ä¸»ç¨‹å¼åŸ·è¡Œå€ ---
if __name__ == "__main__":
    file_path = 'Preprocessing/Data_imputed.csv' # Mac æª”æ¡ˆè·¯å¾‘
    # file_path = 'Preprocessing\Data_imputed.csv' # Windows æª”æ¡ˆè·¯å¾‘
    # è¨­å®šï¼šæ˜¯å¦åŸ·è¡Œæ·±åº¦å­¸ç¿’æ¨¡å‹
    RUN_DEEP_LEARNING = True  # è¨­ç‚º False åªåŸ·è¡ŒåŸºç·šæ¨¡å‹
    
    try:
        print("\n" + "="*30)
        print("GDSC è—¥ç‰©æ•æ„Ÿæ€§é æ¸¬å°ˆæ¡ˆ")
        print("="*30 + "\n")
        
        # 1. è³‡æ–™è™•ç†
        X_train, X_test, y_train, y_test, features = preprocess_data(file_path)
        
        # è½‰æ›ç‚º numpy arrayï¼ˆæ·±åº¦å­¸ç¿’éœ€è¦ï¼‰
        X_train_np = X_train.values if hasattr(X_train, 'values') else X_train
        X_test_np = X_test.values if hasattr(X_test, 'values') else X_test
        y_train_np = y_train.values if hasattr(y_train, 'values') else y_train
        y_test_np = y_test.values if hasattr(y_test, 'values') else y_test
        
        print("\n" + "="*60)
        print("é–‹å§‹è¨“ç·´åŸºç·šæ¨¡å‹ï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰")
        print("="*60)
        
        # 2. åˆ†åˆ¥è¨“ç·´åŸºç·šæ¨¡å‹
        rf_model, rf_pred = train_rf_model(X_train, y_train, X_test)
        xgb_model, xgb_pred = train_xgb_model(X_train, y_train, X_test)
        
        # 3. çµ±ä¸€è©•ä¼°åŸºç·šæ¨¡å‹
        evaluate_and_plot_comparison(y_test, rf_pred, xgb_pred, xgb_model, features)
        
        # 4. æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼ˆé¸æ“‡æ€§åŸ·è¡Œï¼‰
        if RUN_DEEP_LEARNING:
            print("\n\n" + "="*60)
            print("è¨“ç·´æ·±åº¦å­¸ç¿’æ¨¡å‹åŠŸèƒ½å•Ÿç”¨")
            print("="*60)
            
            # â­ æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¨ç«‹çš„è³‡æ–™é è™•ç†ï¼ˆLabel Encodingï¼‰
            # print("\nğŸ’¡ æ³¨æ„ï¼šæ·±åº¦å­¸ç¿’æ¨¡å‹ä½¿ç”¨ç¨ç«‹çš„è³‡æ–™é è™•ç†æµç¨‹ï¼ˆLabel Encodingï¼‰")
            # print("   é€™èˆ‡åŸºç·šæ¨¡å‹çš„ One-Hot Encoding ä¸åŒï¼Œå¯èƒ½ç”¢ç”Ÿæ›´å¥½çš„çµæœ\n")
            
            X_train_dl, X_test_dl, y_train_dl, y_test_dl, features_dl, encoders = \
                preprocess_data_standalone(file_path)
            
            encoder, mlp_model, dl_pred, dl_metrics = run_deep_learning_pipeline(
                X_train_dl, X_test_dl, y_train_dl, y_test_dl, features_dl
            )
            
            print("\n" + "="*30)
            print("æ‰€æœ‰æ¨¡å‹è¨“ç·´å®Œæˆï¼")
            print("="*30)
            
            # æœ€çµ‚æ¯”è¼ƒ
            print("\n" + "="*60)
            print("æœ€çµ‚æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ")
            print("="*60)
            print("\nèªªæ˜ï¼š")
            print("   - Random Forest & XGBoost: ä½¿ç”¨ One-Hot Encoding")
            print("   - Deep Learning: ä½¿ç”¨ Label Encodingï¼ˆç¨ç«‹è³‡æ–™è™•ç†ï¼‰")
            print("   - ç”±æ–¼è³‡æ–™è™•ç†æ–¹å¼ä¸åŒï¼Œæ¸¬è©¦é›†å¯èƒ½ç•¥æœ‰å·®ç•°\n")
            
            rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
            rf_r2 = r2_score(y_test, rf_pred)
            xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
            xgb_r2 = r2_score(y_test, xgb_pred)
            
            print(f"\n{'æ¨¡å‹':<30} {'RMSE':<12} {'RÂ²':<12}")
            print("-"*60)
            print(f"{'Random Forest':<30} {rf_rmse:<12.4f} {rf_r2:<12.4f}")
            print(f"{'XGBoost':<30} {xgb_rmse:<12.4f} {xgb_r2:<12.4f}")
            print(f"{'Deep Learning (Neural Net)':<30} {dl_metrics['RMSE']:<12.4f} {dl_metrics['R2']:<12.4f}")
            print("="*60)
            
            # åˆ¤æ–·æœ€ä½³æ¨¡å‹
            best_model = max(
                [('Random Forest', rf_r2), 
                 ('XGBoost', xgb_r2), 
                 ('Deep Learning', dl_metrics['R2'])],
                key=lambda x: x[1]
            )
            print(f"\næœ€ä½³æ¨¡å‹: {best_model[0]} (RÂ² = {best_model[1]:.4f})")
            
            print("\nç”Ÿæˆçš„æª”æ¡ˆ:")
            print("   åŸºç·šæ¨¡å‹:")
            print("     - model_comparison.png")
            print("   æ·±åº¦å­¸ç¿’æ¨¡å‹:")
            print("     - dl_learning_curves.png")
            print("     - dl_predictions_vs_actual.png")
            print("     - dl_feature_importance.png")
        else:
            print("\nâœ“ åŸºç·šæ¨¡å‹è¨“ç·´å®Œæˆï¼ˆæ·±åº¦å­¸ç¿’æ¨¡å‹å·²è·³éï¼‰")
            print("   å¦‚éœ€åŸ·è¡Œæ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œè«‹è¨­å®š RUN_DEEP_LEARNING = True")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()