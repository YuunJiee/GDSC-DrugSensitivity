# ğŸ§¬ GDSC å°ˆæ¡ˆå®Œæ•´æ¶æ§‹ç¸½çµ

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
GDSC-DrugSensitivity/
â”œâ”€â”€ main.py                          # ä¸»ç¨‹å¼ï¼ˆæ•´åˆæ‰€æœ‰æ¨¡å‹ï¼‰
â”œâ”€â”€ deep_learning_model.py           # æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼ˆç¨ç«‹å¯åŸ·è¡Œï¼‰
â”œâ”€â”€ DEEP_LEARNING_README.md          # æ·±åº¦å­¸ç¿’æ¨¡å‹èªªæ˜æ–‡ä»¶
â”œâ”€â”€ README.md                        # å°ˆæ¡ˆç¸½è¦½
â”œâ”€â”€ requirements.txt                 # Python ä¾è³´å¥—ä»¶
â”œâ”€â”€ activate_env.sh                  # è™›æ“¬ç’°å¢ƒå•Ÿå‹•è…³æœ¬
â”œâ”€â”€ .gitignore                       # Git å¿½ç•¥æ¸…å–®
â”œâ”€â”€ venv/                            # Python è™›æ“¬ç’°å¢ƒ
â””â”€â”€ Preprocessing/
    â”œâ”€â”€ Data_imputed.csv             # è™•ç†å¾Œè³‡æ–™ï¼ˆ37.7 MBï¼‰
    â”œâ”€â”€ Data_imputed_no_meta.csv     # ç„¡å…ƒè³‡æ–™ç‰ˆæœ¬
    â”œâ”€â”€ Preprocessing_v1.ipynb       # å‰è™•ç† Notebook
    â””â”€â”€ Preprocessing.md             # å‰è™•ç†æ–‡ä»¶
```

---

## ğŸ¯ æ¨¡å‹æ¯”è¼ƒç¸½è¦½

| æ¨¡å‹é¡å‹ | æ¨¡å‹åç¨± | æª”æ¡ˆä½ç½® | ä¸»è¦ç‰¹è‰² |
|---------|---------|---------|---------|
| **åŸºç·šæ¨¡å‹** | Random Forest | `main.py` | 100 æ£µæ±ºç­–æ¨¹ï¼Œå¯è§£é‡‹æ€§å¼· |
| **åŸºç·šæ¨¡å‹** | XGBoost | `main.py` | æ¢¯åº¦æå‡ï¼Œæ•ˆèƒ½å„ªç•° |
| **æ·±åº¦å­¸ç¿’** | Autoencoder + MLP | `deep_learning_model.py` | ç‰¹å¾µé™ç¶­ + æ·±åº¦å›æ­¸ |

---

## ğŸ”§ å‡½æ•¸æ¶æ§‹å°ç…§

### `main.py` - åŸºç·šæ¨¡å‹

```python
preprocess_data(file_path)
    â”œâ”€ è®€å– CSV
    â”œâ”€ ç§»é™¤ç„¡é—œæ¬„ä½
    â”œâ”€ One-Hot Encoding
    â””â”€ åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†

train_rf_model(X_train, y_train, X_test)
    â””â”€ è¨“ç·´ Random Forest

train_xgb_model(X_train, y_train, X_test)
    â””â”€ è¨“ç·´ XGBoost

evaluate_and_plot_comparison(y_test, rf_pred, xgb_pred, ...)
    â”œâ”€ è¨ˆç®— RMSE, RÂ²
    â”œâ”€ ç¹ªè£½é æ¸¬åœ–
    â””â”€ è¼¸å‡ºç‰¹å¾µé‡è¦æ€§
```

### `deep_learning_model.py` - æ·±åº¦å­¸ç¿’æ¨¡å‹

```python
build_autoencoder(input_dim, encoding_dim)
    â””â”€ å»ºç«‹ Autoencoder æ¶æ§‹

build_mlp_model(input_dim)
    â””â”€ å»ºç«‹ MLP å›æ­¸æ¶æ§‹

train_autoencoder(X_train, X_val, ...)
    â””â”€ è¨“ç·´ Autoencoderï¼ˆç‰¹å¾µé™ç¶­ï¼‰

train_mlp_model(X_train, y_train, X_val, y_val, ...)
    â””â”€ è¨“ç·´ MLP å›æ­¸æ¨¡å‹

evaluate_dl_model(y_true, y_pred, ...)
    â””â”€ è¨ˆç®— MAE, MSE, RMSE, RÂ², Spearman

calculate_feature_importance(model, encoder, X_test, ...)
    â””â”€ Permutation Importance

plot_learning_curves(ae_history, mlp_history)
    â””â”€ ç¹ªè£½å­¸ç¿’æ›²ç·š

plot_predictions_vs_actual(y_test, y_pred, metrics)
    â””â”€ ç¹ªè£½é æ¸¬ vs å¯¦éš›

plot_feature_importance(importance_df)
    â””â”€ ç¹ªè£½ç‰¹å¾µé‡è¦æ€§

run_deep_learning_pipeline(X_train, X_test, y_train, y_test, features)
    â””â”€ åŸ·è¡Œå®Œæ•´ DL Pipelineï¼ˆä¸»è¦æ¥å£ï¼‰
```

---

## ğŸ“Š è©•ä¼°æŒ‡æ¨™å°ç…§

### åŸºç·šæ¨¡å‹ï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰
- âœ… RMSE (Root Mean Squared Error)
- âœ… RÂ² (R-squared)
- âœ… ç‰¹å¾µé‡è¦æ€§ï¼ˆåŸºæ–¼æ¨¹æ¨¡å‹ï¼‰

### æ·±åº¦å­¸ç¿’æ¨¡å‹
- âœ… MAE (Mean Absolute Error)
- âœ… MSE (Mean Squared Error)
- âœ… RMSE (Root Mean Squared Error)
- âœ… RÂ² (R-squared)
- âœ… **Spearman Correlation**ï¼ˆæ–°å¢ï¼‰
- âœ… ç‰¹å¾µé‡è¦æ€§ï¼ˆPermutation Importanceï¼‰

---

## ğŸ¨ ç”Ÿæˆçš„è¦–è¦ºåŒ–æª”æ¡ˆ

### åŸºç·šæ¨¡å‹
1. **`model_comparison.png`**
   - Random Forest é æ¸¬æ•£é»åœ–
   - XGBoost é æ¸¬æ•£é»åœ–
   - ä¸¦æ’æ¯”è¼ƒ

### æ·±åº¦å­¸ç¿’æ¨¡å‹
1. **`dl_learning_curves.png`**
   - Autoencoder è¨“ç·´/é©—è­‰ loss æ›²ç·š
   - MLP è¨“ç·´/é©—è­‰ loss æ›²ç·š

2. **`dl_predictions_vs_actual.png`**
   - é æ¸¬å€¼ vs å¯¦éš›å€¼æ•£é»åœ–
   - åŒ…å«è©•ä¼°æŒ‡æ¨™æ¨™è¨»

3. **`dl_feature_importance.png`**
   - å‰ 20 å€‹é‡è¦ç‰¹å¾µçš„æ¢å½¢åœ–
   - åŒ…å«èª¤å·®æ¢

---

## ğŸš€ åŸ·è¡Œæµç¨‹

### å®Œæ•´åŸ·è¡Œï¼ˆæ¨è–¦ï¼‰

```bash
# 1. å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source venv/bin/activate

# 2. åŸ·è¡Œä¸»ç¨‹å¼ï¼ˆè‡ªå‹•åŸ·è¡Œæ‰€æœ‰æ¨¡å‹ï¼‰
python main.py
```

**åŸ·è¡Œæµç¨‹ï¼š**
```
1. è®€å–ä¸¦å‰è™•ç†è³‡æ–™
   â†“
2. è¨“ç·´ Random Forest
   â†“
3. è¨“ç·´ XGBoost
   â†“
4. æ¯”è¼ƒåŸºç·šæ¨¡å‹ â†’ ç”Ÿæˆ model_comparison.png
   â†“
5. è¨“ç·´ Autoencoderï¼ˆç‰¹å¾µé™ç¶­ï¼‰
   â†“
6. è¨“ç·´ MLP å›æ­¸æ¨¡å‹
   â†“
7. è©•ä¼°æ·±åº¦å­¸ç¿’æ¨¡å‹
   â†“
8. è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
   â†“
9. ç”Ÿæˆæ‰€æœ‰è¦–è¦ºåŒ–åœ–è¡¨
   â†“
10. é¡¯ç¤ºæœ€çµ‚æ¨¡å‹æ¯”è¼ƒè¡¨
```

### åªåŸ·è¡Œæ·±åº¦å­¸ç¿’

```bash
python deep_learning_model.py
```

### åªåŸ·è¡ŒåŸºç·šæ¨¡å‹

ä¿®æ”¹ `main.py` ç¬¬ 137 è¡Œï¼š
```python
RUN_DEEP_LEARNING = False
```
ç„¶å¾ŒåŸ·è¡Œï¼š
```bash
python main.py
```

---

## ğŸ’¡ ç¨‹å¼ç¢¼é¢¨æ ¼ç‰¹è‰²

### âœ… ç¬¦åˆéœ€æ±‚çš„è¨­è¨ˆ
1. **å‡½æ•¸å¼è¨­è¨ˆ** - æ¯å€‹åŠŸèƒ½éƒ½æ˜¯ç¨ç«‹å‡½æ•¸
2. **æ¨¡çµ„åŒ–** - æ·±åº¦å­¸ç¿’ç¨ç«‹æª”æ¡ˆ
3. **å¯ç¨ç«‹åŸ·è¡Œ** - `deep_learning_model.py` æœ‰ `if __name__ == "__main__"`
4. **çµ±ä¸€é¢¨æ ¼** - èˆ‡ `main.py` çš„å‘½åå’Œçµæ§‹ä¸€è‡´
5. **å®Œæ•´è¨»è§£** - æ¯å€‹å‡½æ•¸éƒ½æœ‰è©³ç´°çš„ docstring

### âœ… æ·±åº¦å­¸ç¿’ç‰¹è‰²
1. **Early Stopping** - é˜²æ­¢éæ“¬åˆ
2. **BatchNormalization** - åŠ é€Ÿè¨“ç·´ã€ç©©å®šæ¢¯åº¦
3. **Dropout** - æ­£å‰‡åŒ–ï¼Œé˜²æ­¢éæ“¬åˆ
4. **L2 Regularization** - æ¬Šé‡æ‡²ç½°
5. **Learning Rate Decay** - è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡
6. **Validation Split** - å¾è¨“ç·´é›†åˆ†å‡ºé©—è­‰é›†

---

## ğŸ“ˆ é æœŸæ•ˆèƒ½

æ ¹æ“š GDSC è³‡æ–™é›†çš„ç‰¹æ€§ï¼š

| æ¨¡å‹ | é æœŸ RÂ² | é æœŸ RMSE | è¨“ç·´æ™‚é–“ |
|------|---------|-----------|----------|
| Random Forest | 0.75 - 0.85 | 0.6 - 0.8 | 3-5 åˆ†é˜ |
| XGBoost | 0.80 - 0.88 | 0.5 - 0.7 | 2-4 åˆ†é˜ |
| Deep Learning | 0.78 - 0.86 | 0.55 - 0.75 | 15-30 åˆ†é˜ |

*å¯¦éš›æ•ˆèƒ½å–æ±ºæ–¼è³‡æ–™å“è³ªå’Œè¶…åƒæ•¸èª¿æ•´*

---

## ğŸ”„ æœªä¾†æ“´å±•å»ºè­°

### æ¨¡å‹æ”¹é€²
- [ ] è©¦é©— Variational Autoencoder (VAE)
- [ ] æ·»åŠ  Attention æ©Ÿåˆ¶
- [ ] å˜—è©¦ Graph Neural Networksï¼ˆåŸºå› äº’å‹•ç¶²çµ¡ï¼‰
- [ ] é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰çµåˆæ‰€æœ‰æ¨¡å‹

### åŠŸèƒ½æ“´å±•
- [ ] è¶…åƒæ•¸è‡ªå‹•èª¿å„ªï¼ˆOptunaï¼‰
- [ ] äº¤å‰é©—è­‰
- [ ] SHAP å€¼åˆ†æï¼ˆå¯è§£é‡‹æ€§ï¼‰
- [ ] æ¨¡å‹åºåˆ—åŒ–èˆ‡è¼‰å…¥
- [ ] Web API éƒ¨ç½²

### è¦–è¦ºåŒ–å¢å¼·
- [ ] æ··æ·†çŸ©é™£ï¼ˆå¦‚æœè½‰ç‚ºåˆ†é¡å•é¡Œï¼‰
- [ ] æ®˜å·®åˆ†æåœ–
- [ ] äº’å‹•å¼åœ–è¡¨ï¼ˆPlotlyï¼‰
- [ ] Dashboardï¼ˆStreamlitï¼‰

---

## ğŸ“ æ•´åˆåˆ°å…¶ä»–ç¨‹å¼

```python
# åŒ¯å…¥æ·±åº¦å­¸ç¿’ pipeline
from deep_learning_model import run_deep_learning_pipeline

# åŸ·è¡Œ
encoder, mlp, predictions, metrics = run_deep_learning_pipeline(
    X_train, X_test, y_train, y_test, feature_names
)

# ä½¿ç”¨çµæœ
print(f"Deep Learning RÂ²: {metrics['R2']:.4f}")
print(f"Spearman Correlation: {metrics['Spearman_Correlation']:.4f}")
```

---

## âœ… å·²å®Œæˆçš„éœ€æ±‚æª¢æŸ¥æ¸…å–®

- [x] **MLP æ¨¡å‹** - 4 å±¤å…¨é€£æ¥ç¶²çµ¡
- [x] **Autoencoder** - ç”¨æ–¼ç‰¹å¾µé™ç¶­
- [x] **å¤šè¼¸å‡ºå›æ­¸** - æ”¯æ´å–®ä¸€ç›®æ¨™ LN_IC50
- [x] **Early Stopping** - é˜²æ­¢éæ“¬åˆ
- [x] **æ­£å‰‡åŒ–** - L2, Dropout, BatchNorm
- [x] **è©•ä¼°æŒ‡æ¨™** - MAE, MSE, RÂ², Spearman
- [x] **è¦–è¦ºåŒ–** - é æ¸¬åœ–ã€å­¸ç¿’æ›²ç·šã€ç‰¹å¾µé‡è¦æ€§
- [x] **å‡½æ•¸å¼è¨­è¨ˆ** - ç¬¦åˆ code style
- [x] **ç¨ç«‹åŸ·è¡Œ** - å¯å–®ç¨æ¸¬è©¦
- [x] **æ•´åˆåˆ° main.py** - çµ±ä¸€åŸ·è¡Œå…¥å£

---

**å°ˆæ¡ˆå·²æº–å‚™å°±ç·’ï¼ğŸ‰**
