"""
æ·±åº¦å­¸ç¿’æ¨¡å‹ - Neural Network Regression
åŸºæ–¼ Kaggle Notebook æ”¹é€²çš„æ–¹æ¡ˆBï¼ˆå®Œæ•´ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. å¯ç¨ç«‹åŸ·è¡Œé€²è¡Œæ¸¬è©¦
2. å¯è¢« main.py èª¿ç”¨ï¼ˆæä¾› run_deep_learning_pipeline å‡½æ•¸ï¼‰
3. åŒ…å«é¡åˆ¥å‹ç‰¹å¾µç·¨ç¢¼ã€æ¨™æº–åŒ–ã€æ·±å±¤ç¶²è·¯ã€callbacks ç­‰å®Œæ•´åŠŸèƒ½

ä½œè€…ï¼šæ”¹ç·¨è‡ª Kaggle - Drug Sensitivity Notebook
æ—¥æœŸï¼š2025-11-30
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings

warnings.filterwarnings('ignore')
plt.style.use('ggplot')

# ==================== GPU æª¢æ¸¬ ====================
def check_gpu_availability():
    """æª¢æŸ¥ä¸¦é¡¯ç¤º GPU ç‹€æ…‹"""
    print("\n" + "="*60)
    print("ğŸ” ç¡¬é«”è¨­å‚™æª¢æ¸¬")
    print("="*60)
    
    print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
    
    # æª¢æŸ¥å¯ç”¨è¨­å‚™
    physical_devices = tf.config.list_physical_devices()
    print(f"\næ‰€æœ‰å¯ç”¨è¨­å‚™:")
    for device in physical_devices:
        print(f"  - {device.device_type}: {device.name}")
    
    # æª¢æŸ¥ GPU
    gpus = tf.config.list_physical_devices('GPU')
    gpu_available = len(gpus) > 0
    
    print(f"\nğŸ–¥ï¸  GPU è¨­å‚™:")
    if gpu_available:
        print(f"  âœ… æ‰¾åˆ° {len(gpus)} å€‹ GPU: {[gpu.name for gpu in gpus]}")
        
        # Apple Silicon ç‰¹æ®Šæç¤º
        import platform
        if platform.processor() == 'arm':
            print(f"\n  ğŸ Apple Silicon åµæ¸¬:")
            print(f"     è™•ç†å™¨: {platform.processor()}")
            print(f"     ç³»çµ±: {platform.system()} {platform.release()}")
            print(f"     âœ… ä½¿ç”¨ Metal å¾Œç«¯é€²è¡Œ GPU åŠ é€Ÿ")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")
    
    print("="*60 + "\n")
    
    return gpu_available

# ==================== è³‡æ–™é è™•ç†ï¼ˆç¨ç«‹åŸ·è¡Œæ¨¡å¼ç”¨ï¼‰ ====================
def preprocess_data_standalone(file_path):
    """
    ç¨ç«‹çš„è³‡æ–™é è™•ç†å‡½æ•¸
    å°ˆé–€ç”¨æ–¼ç¨ç«‹åŸ·è¡Œæ¨¡å¼ï¼Œä¸ä¾è³´ main.py
    
    è™•ç†æµç¨‹ï¼š
    1. è®€å–è³‡æ–™
    2. ç§»é™¤ç¼ºå¤±å€¼
    3. é¡åˆ¥å‹ç‰¹å¾µç·¨ç¢¼
    4. åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
    """
    print(f"\nğŸ“‚ æ­£åœ¨è®€å–è³‡æ–™: {file_path}")
    df = pd.read_csv(file_path)
    
    print(f"   åŸå§‹è³‡æ–™å½¢ç‹€: {df.shape}")
    print(f"   æ¬„ä½æ•¸é‡: {len(df.columns)}")
    
    # ç§»é™¤ç›®æ¨™è®Šæ•¸ç¼ºå¤±å€¼
    df_clean = df.dropna(subset=['LN_IC50'])
    print(f"   ç§»é™¤ç¼ºå¤±å€¼å¾Œ: {df_clean.shape}")
    
    # è™•ç†é¡åˆ¥å‹ç‰¹å¾µ
    print("\nğŸ”„ è™•ç†é¡åˆ¥å‹ç‰¹å¾µ...")
    categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()
    print(f"   ç™¼ç¾ {len(categorical_cols)} å€‹é¡åˆ¥å‹æ¬„ä½")
    
    # ç§»é™¤ä¸éœ€è¦çš„æ¬„ä½ï¼ˆIDã€åç¨±ç­‰ï¼‰
    exclude_cols = ['COSMIC_ID', 'CELL_LINE_NAME', 'DRUG_NAME', 'DRUG_ID']
    
    # å°é¡åˆ¥å‹ç‰¹å¾µé€²è¡Œ Label Encoding
    label_encoders = {}
    for col in categorical_cols:
        if col not in exclude_cols and col in df_clean.columns:
            le = LabelEncoder()
            df_clean[col] = le.fit_transform(df_clean[col].astype(str))
            label_encoders[col] = le
            print(f"     âœ“ {col}: {len(le.classes_)} å€‹é¡åˆ¥")
    
    # é¸æ“‡ç‰¹å¾µï¼ˆæ’é™¤IDã€åç¨±å’Œç›®æ¨™è®Šæ•¸ï¼‰
    drop_cols = [col for col in exclude_cols if col in df_clean.columns] + ['LN_IC50']
    features = df_clean.drop(columns=drop_cols)
    target = df_clean['LN_IC50']
    
    feature_names = features.columns.tolist()
    
    print(f"\nğŸ“Š æœ€çµ‚ç‰¹å¾µæ•¸é‡: {len(feature_names)}")
    print(f"   æ•¸å€¼å‹ç‰¹å¾µ: {len(features.select_dtypes(include=[np.number]).columns)}")
    
    # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(
        features, target, test_size=0.2, random_state=42
    )
    
    print(f"\nâœ… è³‡æ–™åˆ†å‰²å®Œæˆ:")
    print(f"   è¨“ç·´é›†: {X_train.shape}")
    print(f"   æ¸¬è©¦é›†: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test, feature_names, label_encoders

# ==================== å»ºç«‹æ·±åº¦å­¸ç¿’æ¨¡å‹ ====================
def build_neural_network(input_dim):
    """
    å»ºç«‹æ·±åº¦ç¥ç¶“ç¶²è·¯æ¨¡å‹ï¼ˆæ–¹æ¡ˆBï¼šå®Œæ•´ç‰ˆï¼‰
    
    æ¶æ§‹ï¼š
    - è¼¸å…¥å±¤
    - Dense(256) + BatchNorm + ReLU + Dropout(0.3)
    - Dense(128) + BatchNorm + ReLU + Dropout(0.3)
    - Dense(64) + ReLU + Dropout(0.2)
    - è¼¸å‡ºå±¤ (1å€‹ç¥ç¶“å…ƒï¼Œç·šæ€§æ¿€æ´»)
    
    åƒæ•¸ï¼š
        input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
    
    è¿”å›ï¼š
        ç·¨è­¯å¥½çš„ Keras Sequential æ¨¡å‹
    """
    model = Sequential([
        # ç¬¬ä¸€å±¤ï¼š256 ç¥ç¶“å…ƒ
        Dense(256, activation='relu', input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.3),
        
        # ç¬¬äºŒå±¤ï¼š128 ç¥ç¶“å…ƒ
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        # ç¬¬ä¸‰å±¤ï¼š64 ç¥ç¶“å…ƒ
        Dense(64, activation='relu'),
        Dropout(0.2),
        
        # è¼¸å‡ºå±¤ï¼šå›æ­¸ä»»å‹™
        Dense(1, activation='linear')
    ], name='Neural_Network_Regression')
    
    # ç·¨è­¯æ¨¡å‹
    model.compile(
        optimizer='adam',
        loss='mean_squared_error',
        metrics=['mean_absolute_error', 'mse']
    )
    
    return model

# ==================== è¨“ç·´æ¨¡å‹ ====================
def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, verbose=1):
    """
    è¨“ç·´æ·±åº¦å­¸ç¿’æ¨¡å‹
    
    åƒæ•¸ï¼š
        model: Keras æ¨¡å‹
        X_train, y_train: è¨“ç·´è³‡æ–™
        X_val, y_val: é©—è­‰è³‡æ–™
        epochs: è¨“ç·´è¼ªæ•¸
        batch_size: æ‰¹æ¬¡å¤§å°
        verbose: è¨“ç·´éç¨‹è¼¸å‡ºè©³ç´°ç¨‹åº¦
    
    è¿”å›ï¼š
        history: è¨“ç·´æ­·å²è¨˜éŒ„
    """
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
    print(f"   Epochs: {epochs}")
    print(f"   Batch Size: {batch_size}")
    print(f"   è¨“ç·´æ¨£æœ¬: {len(y_train)}")
    print(f"   é©—è­‰æ¨£æœ¬: {len(y_val)}")
    
    # è¨­å®š Callbacks
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    
    # è¨“ç·´æ¨¡å‹
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop, reduce_lr],
        verbose=verbose
    )
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼")
    
    return history

# ==================== è©•ä¼°æ¨¡å‹ ====================
def evaluate_model(model, X_test, y_test, model_name="Neural Network"):
    """
    è©•ä¼°æ¨¡å‹æ•ˆèƒ½
    
    åƒæ•¸ï¼š
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        X_test, y_test: æ¸¬è©¦è³‡æ–™
        model_name: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰
    
    è¿”å›ï¼š
        y_pred: é æ¸¬çµæœ
        metrics: è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    print(f"\nğŸ“Š è©•ä¼° {model_name} æ•ˆèƒ½...")
    
    # é æ¸¬
    y_pred = model.predict(X_test, verbose=0).flatten()
    
    # è¨ˆç®—æŒ‡æ¨™
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Spearman ç›¸é—œä¿‚æ•¸
    spearman_corr, spearman_pval = spearmanr(y_test, y_pred)
    
    # é¡¯ç¤ºçµæœ
    print("\n" + "="*60)
    print(f"{model_name} è©•ä¼°çµæœ")
    print("="*60)
    print(f"  RÂ² Score:              {r2:.4f}")
    print(f"  RMSE:                  {rmse:.4f}")
    print(f"  MAE:                   {mae:.4f}")
    print(f"  MSE:                   {mse:.4f}")
    print(f"  Spearman Correlation:  {spearman_corr:.4f} (p={spearman_pval:.4e})")
    print("="*60)
    
    metrics = {
        'R2': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MSE': mse,
        'Spearman_Correlation': spearman_corr,
        'Spearman_PValue': spearman_pval
    }
    
    return y_pred, metrics

# ==================== è¦–è¦ºåŒ–çµæœ ====================
def plot_training_history(history, save_path='dl_learning_curves.png'):
    """ç¹ªè£½è¨“ç·´éç¨‹æ›²ç·š"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # MAE æ›²ç·š
    ax1.plot(history.history['mean_absolute_error'], label='Train MAE', linewidth=2)
    ax1.plot(history.history['val_mean_absolute_error'], label='Val MAE', linewidth=2)
    ax1.set_title('Model MAE over Epochs', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('MAE', fontsize=12)
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # Loss æ›²ç·š
    ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)
    ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)
    ax2.set_title('Model Loss (MSE) over Epochs', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Loss (MSE)', fontsize=12)
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ è¨“ç·´æ›²ç·šå·²ä¿å­˜è‡³: {save_path}")
    plt.close()

def plot_predictions(y_test, y_pred, metrics, save_path='dl_predictions_vs_actual.png'):
    """ç¹ªè£½é æ¸¬ vs å¯¦éš›å€¼æ•£é»åœ–"""
    plt.figure(figsize=(10, 8))
    
    # æ•£é»åœ–
    plt.scatter(y_test, y_pred, alpha=0.5, s=20, color='steelblue', edgecolors='navy', linewidth=0.5)
    
    # å®Œç¾é æ¸¬ç·š
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
    
    # æ¨™é¡Œå’Œæ¨™ç±¤
    plt.title(f'Predictions vs Actual IC50 Values\nRÂ² = {metrics["R2"]:.4f}, RMSE = {metrics["RMSE"]:.4f}',
              fontsize=14, fontweight='bold')
    plt.xlabel('Actual LN_IC50', fontsize=12)
    plt.ylabel('Predicted LN_IC50', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ çµ±è¨ˆè³‡è¨Š
    textstr = f'MAE = {metrics["MAE"]:.4f}\nSpearman Ï = {metrics["Spearman_Correlation"]:.4f}'
    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,
             fontsize=10, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ é æ¸¬æ•£é»åœ–å·²ä¿å­˜è‡³: {save_path}")
    plt.close()

def plot_feature_importance(model, feature_names, top_n=20, save_path='dl_feature_importance.png'):
    """
    è¨ˆç®—ä¸¦ç¹ªè£½ç‰¹å¾µé‡è¦æ€§ï¼ˆåŸºæ–¼æ¬Šé‡ï¼‰
    
    æ³¨æ„ï¼šæ·±åº¦å­¸ç¿’çš„ç‰¹å¾µé‡è¦æ€§è¨ˆç®—è¼ƒç‚ºè¤‡é›œï¼Œ
    é€™è£¡ä½¿ç”¨ç¬¬ä¸€å±¤æ¬Šé‡çš„çµ•å°å€¼å¹³å‡ä½œç‚ºè¿‘ä¼¼å€¼
    """
    print(f"\nğŸ“ˆ è¨ˆç®—ç‰¹å¾µé‡è¦æ€§ï¼ˆTop {top_n}ï¼‰...")
    
    try:
        # ç²å–ç¬¬ä¸€å±¤çš„æ¬Šé‡
        first_layer_weights = model.layers[0].get_weights()[0]  # shape: (n_features, n_neurons)
        
        # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„é‡è¦æ€§ï¼ˆæ¬Šé‡çµ•å°å€¼çš„å¹³å‡ï¼‰
        feature_importance = np.abs(first_layer_weights).mean(axis=1)
        
        # å‰µå»º DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False).head(top_n)
        
        # ç¹ªåœ–
        plt.figure(figsize=(12, 8))
        plt.barh(range(len(importance_df)), importance_df['importance'], color='skyblue', edgecolor='navy')
        plt.yticks(range(len(importance_df)), importance_df['feature'])
        plt.xlabel('Importance (Absolute Weight Mean)', fontsize=12)
        plt.title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ ç‰¹å¾µé‡è¦æ€§åœ–å·²ä¿å­˜è‡³: {save_path}")
        plt.close()
        
        # é¡¯ç¤º Top 10
        print("\n" + "="*60)
        print(f"Top 10 Most Important Features")
        print("="*60)
        print(importance_df.head(10).to_string(index=False))
        print("="*60)
        
    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾µé‡è¦æ€§è¨ˆç®—å¤±æ•—: {e}")

# ==================== ä¸»è¦ Pipeline å‡½æ•¸ï¼ˆä¾› main.py èª¿ç”¨ï¼‰ ====================
def run_deep_learning_pipeline(X_train, X_test, y_train, y_test, feature_names):
    """
    åŸ·è¡Œå®Œæ•´çš„æ·±åº¦å­¸ç¿’ Pipeline
    
    é€™æ˜¯ä¾› main.py èª¿ç”¨çš„ä¸»è¦æ¥å£å‡½æ•¸
    
    åƒæ•¸ï¼š
        X_train, X_test: è¨“ç·´/æ¸¬è©¦ç‰¹å¾µï¼ˆDataFrame æˆ– Arrayï¼‰
        y_train, y_test: è¨“ç·´/æ¸¬è©¦ç›®æ¨™è®Šæ•¸ï¼ˆSeries æˆ– Arrayï¼‰
        feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨
    
    è¿”å›ï¼š
        encoder: ç·¨ç¢¼å™¨ï¼ˆé€™è£¡è¿”å› scalerï¼‰
        model: è¨“ç·´å¥½çš„æ·±åº¦å­¸ç¿’æ¨¡å‹
        y_pred: æ¸¬è©¦é›†é æ¸¬çµæœ
        metrics: è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    print("\n" + "="*60)
    print("æ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´é–‹å§‹")
    print("="*60)
    
    # æª¢æŸ¥ GPU
    check_gpu_availability()
    
    # 1. ç¢ºä¿è³‡æ–™æ ¼å¼æ­£ç¢ºï¼ˆè½‰æ›ç‚º numpy arrayï¼‰
    print("\n[1/7] è³‡æ–™æ ¼å¼è½‰æ›...")
    if hasattr(X_train, 'values'):
        X_train = X_train.values
    if hasattr(X_test, 'values'):
        X_test = X_test.values
    if hasattr(y_train, 'values'):
        y_train = y_train.values
    if hasattr(y_test, 'values'):
        y_test = y_test.values
    
    print(f"   è¨“ç·´é›†å½¢ç‹€: {X_train.shape}")
    print(f"   æ¸¬è©¦é›†å½¢ç‹€: {X_test.shape}")
    
    # 2. ç‰¹å¾µæ¨™æº–åŒ–
    print("\n[2/7] ç‰¹å¾µæ¨™æº–åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"   æ¨™æº–åŒ–å¾Œç¯„åœ: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
    
    # 3. é©—è­‰é›†åˆ‡åˆ†
    print("\n[3/7] åˆ‡åˆ†é©—è­‰é›†...")
    X_train_final, X_val, y_train_final, y_val = train_test_split(
        X_train_scaled, y_train, test_size=0.2, random_state=42
    )
    
    print(f"   æœ€çµ‚è¨“ç·´é›†: {X_train_final.shape}")
    print(f"   é©—è­‰é›†:     {X_val.shape}")
    
    # 4. å»ºç«‹æ¨¡å‹
    print("\n[4/7] å»ºç«‹ç¥ç¶“ç¶²è·¯æ¨¡å‹...")
    input_dim = X_train_scaled.shape[1]
    model = build_neural_network(input_dim)
    
    print("\næ¨¡å‹æ¶æ§‹:")
    model.summary()
    
    # 5. è¨“ç·´æ¨¡å‹
    print("\n[5/7] è¨“ç·´æ¨¡å‹...")
    history = train_model(
        model,
        X_train_final, y_train_final,
        X_val, y_val,
        epochs=100,
        batch_size=64,
        verbose=1
    )
    
    # 6. è©•ä¼°æ¨¡å‹
    print("\n[6/7] è©•ä¼°æ¨¡å‹...")
    y_pred, metrics = evaluate_model(model, X_test_scaled, y_test)
    
    # 7. è¦–è¦ºåŒ–çµæœ
    print("\n[7/7] ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    plot_training_history(history)
    plot_predictions(y_test, y_pred, metrics)
    plot_feature_importance(model, feature_names)
    
    print("\n" + "="*60)
    print("âœ… æ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´å®Œæˆï¼")
    print("="*60)
    print("\nğŸ“Š ç”Ÿæˆçš„æª”æ¡ˆ:")
    print("   - dl_learning_curves.png")
    print("   - dl_predictions_vs_actual.png")
    print("   - dl_feature_importance.png")
    
    # è¿”å›çµæœï¼ˆç¬¦åˆ main.py çš„æ¥å£ï¼‰
    return scaler, model, y_pred, metrics

# ==================== ç¨ç«‹åŸ·è¡Œæ¨¡å¼ ====================
if __name__ == "__main__":
    """
    ç¨ç«‹åŸ·è¡Œæ­¤æª”æ¡ˆé€²è¡Œæ¸¬è©¦
    
    ç”¨æ³•ï¼š
        python DL/deep_learning_model.py
    """
    print("\n" + "="*30)
    print("æ·±åº¦å­¸ç¿’æ¨¡å‹ - ç¨ç«‹æ¸¬è©¦æ¨¡å¼")
    print(""*30)
    
    file_path = '../Preprocessing/Data_imputed.csv'  # ç›¸å°æ–¼ DL è³‡æ–™å¤¾çš„è·¯å¾‘
    
    try:
        # 1. è³‡æ–™é è™•ç†
        X_train, X_test, y_train, y_test, features, encoders = preprocess_data_standalone(file_path)
        
        # 2. åŸ·è¡Œæ·±åº¦å­¸ç¿’ Pipeline
        scaler, model, y_pred, metrics = run_deep_learning_pipeline(
            X_train, X_test, y_train, y_test, features
        )
        
        # 3. å®Œæˆæç¤º
        print("\n" + "="*30)
        print("æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
        print("="*30)
        
        print("\næ¨¡å‹æ•ˆèƒ½ç¸½çµ:")
        print(f"   RÂ² Score: {metrics['R2']:.4f}")
        print(f"   RMSE:     {metrics['RMSE']:.4f}")
        print(f"   MAE:      {metrics['MAE']:.4f}")
        
    except FileNotFoundError:
        print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ '{file_path}'")
        print("   è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢º")
        print("   å¦‚æœå¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œï¼Œè«‹ä½¿ç”¨: python DL/deep_learning_model.py")
        
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


# æ¨¡å‹æ•ˆèƒ½ç¸½çµ:
#    RÂ² Score: 0.6189
#    RMSE:     1.7052
#    MAE:      1.1486