import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from sklearn.metrics import mean_squared_error, r2_score

# å°å…¥é‡æ§‹å¾Œçš„æ¨¡çµ„
from src.data.preprocessing import load_raw_data, load_and_preprocess_baseline, load_and_preprocess_dl
from src.data.eda import perform_eda
from src.models.baseline import train_rf_model, train_xgb_model
from src.models.deep_learning.deep_learning_model import run_deep_learning_pipeline
from src.utils.evaluation import calculate_metrics
from src.utils.visualization import plot_comparison, plot_residuals, plot_training_history
from src.utils.explainability import analyze_branch_importance, analyze_shap_values
# Import optuna tuning module
from src.optimization.optuna_tuning import run_optimization

# è¨­å®šç¹ªåœ–é¢¨æ ¼
plt.style.use('ggplot')

def run_experiment(file_path, include_ids, run_baseline=True, run_dl=False):
    """
    åŸ·è¡Œå–®æ¬¡å¯¦é©—
    :param include_ids: æ˜¯å¦åŒ…å« ID ç‰¹å¾µ
    :param run_baseline: æ˜¯å¦åŸ·è¡ŒåŸºç·šæ¨¡å‹
    :param run_dl: æ˜¯å¦åŸ·è¡Œæ·±åº¦å­¸ç¿’
    :return: å¯¦é©—çµæœå­—å…¸
    """
    mode_name = "With_IDs" if include_ids else "No_IDs"
    print(f"\n{'='*80}")
    print(f"ğŸš€ é–‹å§‹å¯¦é©—: {mode_name} (åŒ…å« ID: {include_ids})")
    print(f"{'='*80}")

    results = {}

    # 1. åŸºç·šæ¨¡å‹æµç¨‹
    if run_baseline:
        print(f"\n--- [Baseline] è³‡æ–™è™•ç† ({mode_name}) ---")
        X_train, X_test, y_train, y_test, features = load_and_preprocess_baseline(
            file_path, variance_threshold=0.01, include_ids=include_ids
        )
        
        # 1.2 è¨“ç·´æ¨¡å‹
        rf_model, rf_pred = train_rf_model(X_train, y_train, X_test)
        xgb_model, xgb_pred = train_xgb_model(X_train, y_train, X_test)
        
        # 1.3 è©•ä¼°
        rf_rmse, rf_r2, _ = calculate_metrics(y_test, rf_pred, f"Random Forest ({mode_name})")
        xgb_rmse, xgb_r2, _ = calculate_metrics(y_test, xgb_pred, f"XGBoost ({mode_name})")
        
        # ç¹ªè£½æ®˜å·®åœ–
        plot_residuals(y_test, rf_pred, f"Random Forest ({mode_name})", f"results/figures/residuals_rf_{mode_name}.png")
        plot_residuals(y_test, xgb_pred, f"XGBoost ({mode_name})", f"results/figures/residuals_xgb_{mode_name}.png")
        
        results['RF'] = {'RMSE': rf_rmse, 'R2': rf_r2}
        results['XGB'] = {'RMSE': xgb_rmse, 'R2': xgb_r2}

        # é¡¯ç¤º XGB ç‰¹å¾µé‡è¦æ€§ (åƒ…é¡¯ç¤ºå‰ 5 å€‹ï¼Œé¿å…æ´—ç‰ˆ)
        feat_imp = pd.DataFrame({
            'feature': features,
            'importance': xgb_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\n[XGBoost - {mode_name}] Top 5 ç‰¹å¾µ:\n", feat_imp.head(5))
        
        # å„²å­˜ç‰¹å¾µé‡è¦æ€§è¡¨
        os.makedirs('results/tables', exist_ok=True)
        feat_imp.head(50).to_csv(f'results/tables/feature_importance_xgb_{mode_name}.csv', index=False)
        print(f"   -> å·²å„²å­˜ç‰¹å¾µé‡è¦æ€§è¡¨: results/tables/feature_importance_xgb_{mode_name}.csv")

    # 2. æ·±åº¦å­¸ç¿’æ¨¡å‹æµç¨‹
    if run_dl:
        print(f"\n--- [Deep Learning] è¨“ç·´ ({mode_name}) ---")
        
        # 2.1 è³‡æ–™è™•ç† (DL)
        X_cell_train, X_drug_train, X_cell_test, X_drug_test, y_train_dl, y_test_dl, features_dl, encoders = \
            load_and_preprocess_dl(file_path, include_ids=include_ids)
        
        # 2.2 åŸ·è¡Œ Pipeline
        scalers, mlp_model, dl_pred, dl_metrics, history = run_deep_learning_pipeline(
            X_cell_train, X_drug_train, X_cell_test, X_drug_test, y_train_dl, y_test_dl, features_dl
        )
        
        # ç¹ªè£½å­¸ç¿’æ›²ç·š
        plot_training_history(history, f"results/figures/dl_learning_curves_{mode_name}.png")
        
        # --- æ–°å¢: å¯è§£é‡‹æ€§åˆ†æ (Explainability) ---
        # 1. Macro-Level: Branch Importance
        analyze_branch_importance(mlp_model, f"results/figures/dl_branch_importance_{mode_name}.png")
        
        # 2. Micro-Level: SHAP Values
        # æ³¨æ„: é€™è£¡å‚³å…¥çš„æ˜¯åŸå§‹æ•¸æ“šçš„ listï¼ŒSHAP æœƒè‡ªå·±è™•ç†
        analyze_shap_values(
            mlp_model, 
            [X_cell_train, X_drug_train], 
            [X_cell_test, X_drug_test], 
            features_dl, 
            f"results/figures/dl_shap_summary_{mode_name}.png"
        )
        
        results['DL'] = {'RMSE': dl_metrics['RMSE'], 'R2': dl_metrics['R2']}
        
        # ç¹ªè£½æ®˜å·®åœ–
        plot_residuals(y_test_dl, dl_pred, f"Deep Learning ({mode_name})", f"results/figures/residuals_dl_{mode_name}.png")

    # å„²å­˜æ¨¡å‹è©•ä¼°æŒ‡æ¨™ (Merge with existing if available)
    metrics_path = f'results/tables/metrics_{mode_name}.csv'
    if os.path.exists(metrics_path):
        try:
            existing_df = pd.read_csv(metrics_path, index_col=0)
            existing_results = existing_df.T.to_dict()
            # Update existing results with new ones
            existing_results.update(results)
            results = existing_results
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å–èˆŠçš„çµæœæª”æ¡ˆï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ: {e}")

    metrics_df = pd.DataFrame(results).T
    metrics_df.to_csv(metrics_path)
    print(f"   -> å·²å„²å­˜æ¨¡å‹è©•ä¼°æŒ‡æ¨™: {metrics_path}")

    return results

def plot_experiment_comparison(results_with_ids, results_no_ids):
    """ç¹ªè£½å¯¦é©—å°æ¯”åœ–"""
    # å‹•æ…‹åµæ¸¬æœ‰å“ªäº›æ¨¡å‹
    all_models = set(results_with_ids.keys()) | set(results_no_ids.keys())
    # æ’åºï¼šRF, XGB, DL (å¦‚æœå­˜åœ¨)
    preferred_order = ['RF', 'XGB', 'DL']
    models = [m for m in preferred_order if m in all_models]
    # åŠ å…¥å…¶ä»–å¯èƒ½çš„æ¨¡å‹
    for m in all_models:
        if m not in models:
            models.append(m)
    
    if not models:
        print("âš ï¸ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹çµæœé€²è¡Œç¹ªåœ–ã€‚")
        return

    r2_with_ids = []
    r2_no_ids = []
    
    for m in models:
        r2_with_ids.append(results_with_ids.get(m, {}).get('R2', 0))
        r2_no_ids.append(results_no_ids.get(m, {}).get('R2', 0))
    
    x = np.arange(len(models))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, r2_with_ids, width, label='With IDs (Memorization)', color='#3498db')
    rects2 = ax.bar(x + width/2, r2_no_ids, width, label='No IDs (Generalization)', color='#e74c3c')
    
    ax.set_ylabel('RÂ² Score')
    ax.set_title('Impact of ID Features on Model Performance')
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.legend()
    
    ax.bar_label(rects1, padding=3, fmt='%.2f')
    ax.bar_label(rects2, padding=3, fmt='%.2f')
    
    plt.tight_layout()
    plt.savefig('experiment_comparison.png')
    print("\nğŸ“Š å°æ¯”åœ–è¡¨å·²å„²å­˜ç‚º: experiment_comparison.png")
    # plt.show() # å¦‚æœåœ¨éäº’å‹•ç’°å¢ƒå¯è¨»è§£æ‰

import argparse

def load_saved_results(mode_name):
    """å¾ CSV è¼‰å…¥å„²å­˜çš„å¯¦é©—çµæœ"""
    path = f'results/tables/metrics_{mode_name}.csv'
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ° {mode_name} çš„çµæœæª”æ¡ˆ ({path})ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒã€‚")
        return {}
    
    try:
        df = pd.read_csv(path, index_col=0)
        return df.T.to_dict()
    except Exception as e:
        print(f"âš ï¸ è®€å– {mode_name} çµæœå¤±æ•—: {e}")
        return {}

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GDSC Drug Sensitivity Prediction Pipeline")
    parser.add_argument('--eda', action='store_true', help='Perform EDA')
    parser.add_argument('--baseline', action='store_true', help='Run Baseline Models')
    parser.add_argument('--dl', action='store_true', help='Run Deep Learning Model')
    parser.add_argument('--optimize', action='store_true', help='Run Optimization')
    parser.add_argument('--compare', action='store_true', help='Compare Results')
    parser.add_argument('--viz-features', action='store_true', help='Visualize Features')
    parser.add_argument('--all', action='store_true', help='Run All Steps')
    parser.add_argument('--viz-with-id', action='store_true', help='Experiment With IDs')
    parser.add_argument('--viz-no-id', action='store_true', help='Experiment Without IDs')
    
    args = parser.parse_args()
    file_path = 'data/raw/GDSC_DATASET.csv'
    
    try:
        if args.optimize:
            try:
                print("\n=== å„ªåŒ–æ¨¡å¼ ===")
                print("1. Deep Learning (dl)")
                print("2. Random Forest (rf)")
                print("3. XGBoost (xgb)")
                model_choice = input("è¼¸å…¥æ¨¡å‹ä»£è™Ÿ (dl/rf/xgb) [é è¨­: dl]: ").strip().lower()
                if not model_choice: model_choice = 'dl'
                if model_choice not in ['dl', 'rf', 'xgb']: model_choice = 'dl'
                
                run_optimization(file_path, include_ids=True, model_type=model_choice)
            except KeyboardInterrupt:
                print("\nå„ªåŒ–å·²å–æ¶ˆã€‚")
        # 0. åŸ·è¡Œ EDA
        if args.eda or args.all:
            print("Step 0: åŸ·è¡Œæ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA)...")
            # ä½¿ç”¨åŸå§‹è³‡æ–™é€²è¡Œ EDAï¼Œä»¥å‘ˆç¾çœŸå¯¦çš„ç¼ºå¤±å€¼ç‹€æ³
            raw_df = load_raw_data('data/raw/GDSC_DATASET.csv')
            perform_eda(raw_df)
        
        results_with_ids = {}
        results_no_ids = {}

        # 1. åŸ·è¡Œå¯¦é©— (Baseline & DL)
        # ç‚ºäº†é¿å…é‡è¤‡è¼‰å…¥è³‡æ–™ï¼Œæˆ‘å€‘å°‡ Baseline å’Œ DL çš„åŸ·è¡Œé‚è¼¯æ•´åˆåœ¨ run_experiment ä¸­
        # ä½†é€éåƒæ•¸æ§åˆ¶æ˜¯å¦åŸ·è¡Œ DL
        
        run_baseline = args.baseline or args.all
        run_dl = args.dl or args.all
        
        if run_baseline or run_dl:
            # Determine which modes to run
            # Default to running both
            do_with_id = True
            do_no_id = True
            
            # If specific mode flags are provided, they override the default "run both" behavior
            if args.viz_with_id or args.viz_no_id: # Changed from args.with_id / args.no_id to viz flags
                do_with_id = args.viz_with_id
                do_no_id = args.viz_no_id

            # å¯¦é©— 1: åŒ…å« ID (é«˜åˆ†æ¨¡å¼)
            if do_with_id:
                print(f"\nğŸš€ åŸ·è¡Œå¯¦é©—: With_IDs (Baseline: {run_baseline}, DL: {run_dl})")
                results_with_ids = run_experiment(file_path, include_ids=True, run_baseline=run_baseline, run_dl=run_dl)
            
            # å¯¦é©— 2: ä¸åŒ…å« ID (æ³›åŒ–æ¨¡å¼)
            if do_no_id:
                print(f"\nğŸš€ åŸ·è¡Œå¯¦é©—: No_IDs (Baseline: {run_baseline}, DL: {run_dl})")
                results_no_ids = run_experiment(file_path, include_ids=False, run_baseline=run_baseline, run_dl=run_dl)
        
        # 2. åŸ·è¡Œæ¯”è¼ƒ
        if args.compare or args.all:
            print("\nStep 3: åŸ·è¡Œæ¨¡å‹æ¯”è¼ƒ...")
            
            # å¦‚æœçµæœå­—å…¸æ˜¯ç©ºçš„ (ä»£è¡¨é€™æ¬¡æ²’è·‘æ¨¡å‹)ï¼Œå˜—è©¦å¾æª”æ¡ˆè¼‰å…¥
            if not results_with_ids:
                print("   -> å˜—è©¦è¼‰å…¥ With_IDs æ­·å²çµæœ...")
                results_with_ids = load_saved_results("With_IDs")
            
            if not results_no_ids:
                print("   -> å˜—è©¦è¼‰å…¥ No_IDs æ­·å²çµæœ...")
                results_no_ids = load_saved_results("No_IDs")
                
            if results_with_ids and results_no_ids:
                # ç¹ªè£½æ¯”è¼ƒåœ–
                plot_experiment_comparison(results_with_ids, results_no_ids)
                
                # å„²å­˜æœ€çµ‚å¯¦é©—ç¸½çµè¡¨
                summary_data = []
                # å–å¾—æ‰€æœ‰æ¨¡å‹åç¨± (è¯é›†)
                all_models = set(results_with_ids.keys()) | set(results_no_ids.keys())
                
                for model in all_models:
                     row = {'Model': model}
                     if model in results_with_ids:
                         row['With_IDs_R2'] = results_with_ids[model]['R2']
                         row['With_IDs_RMSE'] = results_with_ids[model]['RMSE']
                     if model in results_no_ids:
                         row['No_IDs_R2'] = results_no_ids[model]['R2']
                         row['No_IDs_RMSE'] = results_no_ids[model]['RMSE']
                     summary_data.append(row)
                
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_csv('results/tables/final_experiment_summary.csv', index=False)
                print(f"\nâœ… æœ€çµ‚å¯¦é©—ç¸½çµè¡¨å·²å„²å­˜: results/tables/final_experiment_summary.csv")
            else:
                 # è‹¥ç„¡æ³•æ¯”è¼ƒï¼ˆä¾‹å¦‚åªè·‘äº†å…¶ä¸­ä¸€ç¨®ï¼‰ï¼Œè‡³å°‘å°å‡ºè¨Šæ¯
                 pass

        # ç¨ç«‹è¦–è¦ºåŒ–åŠŸèƒ½
        should_viz = args.viz_features or args.all
        # å¦‚æœä½¿ç”¨è€…åªæŒ‡å®šäº† --viz-with-id æˆ– --viz-no-id ä½†æ²’æœ‰æŒ‡å®šåŸ·è¡Œå¯¦é©— (--baseline/--dl)ï¼Œå‰‡å‡è¨­æ˜¯ç‚ºäº†è¦–è¦ºåŒ–
        if (args.viz_with_id or args.viz_no_id) and not (args.baseline or args.dl):
            should_viz = True

        if should_viz:
            from src.utils.visualization import plot_feature_importance, plot_single_mode_performance_bar
            
            print("\nğŸ“Š æ­£åœ¨ç¹ªè£½è¦–è¦ºåŒ–åœ–è¡¨...")
            
            # æ±ºå®šè¦ç•«å“ªäº›
            show_no_id = True
            show_with_id = True
            
            # å¦‚æœæœ‰æŒ‡å®šç‰¹å®šæ¨¡å¼ï¼Œå‰‡åªç•«è©²æ¨¡å¼ (é™¤éæ˜¯ viz-features é€™ç¨®å¤§é–‹é—œ)
            if (args.viz_with_id or args.viz_no_id) and not args.viz_features and not args.all:
                 show_no_id = args.viz_no_id
                 show_with_id = args.viz_with_id

            # --- 1. ç‰¹å¾µé‡è¦æ€§ & R2 è¡¨ç¾åœ– (No_IDs) ---
            if show_no_id:
                # Feature Importance
                csv_path_no = 'results/tables/feature_importance_xgb_No_IDs.csv'
                if os.path.exists(csv_path_no):
                    try:
                        df = pd.read_csv(csv_path_no)
                        plot_feature_importance(df['feature'], df['importance'], top_n=20, save_path='results/figures/feature_importance_xgb_No_IDs.png')
                        print("   -> No_IDs ç‰¹å¾µåœ–å·²æ›´æ–°")
                    except Exception as e:
                        print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ No_IDs ç‰¹å¾µåœ–: {e}")
                elif args.viz_no_id:
                     print(f"   âš ï¸ æ‰¾ä¸åˆ° No_IDs ç‰¹å¾µçµæœ ({csv_path_no})")
                
                # R2 Score Bar Chart
                metrics_no = load_saved_results("No_IDs")
                if metrics_no:
                    try:
                        plot_single_mode_performance_bar(metrics_no, "No_IDs", 'results/figures/performance_bar_No_IDs.png')
                    except Exception as e:
                        print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ No_IDs R2 åœ–: {e}")

            # --- 2. ç‰¹å¾µé‡è¦æ€§ & R2 è¡¨ç¾åœ– (With_IDs) ---
            if show_with_id:
                # Feature Importance
                csv_path_with = 'results/tables/feature_importance_xgb_With_IDs.csv'
                if os.path.exists(csv_path_with):
                    try:
                        df = pd.read_csv(csv_path_with)
                        plot_feature_importance(df['feature'], df['importance'], top_n=20, save_path='results/figures/feature_importance_xgb_With_IDs.png')
                        print("   -> With_IDs ç‰¹å¾µåœ–å·²æ›´æ–°")
                    except Exception as e:
                        print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ With_IDs ç‰¹å¾µåœ–: {e}")
                elif args.viz_with_id:
                     print(f"   âš ï¸ æ‰¾ä¸åˆ° With_IDs ç‰¹å¾µçµæœ ({csv_path_with})")
                
                # R2 Score Bar Chart
                metrics_with = load_saved_results("With_IDs")
                if metrics_with:
                    try:
                        plot_single_mode_performance_bar(metrics_with, "With_IDs", 'results/figures/performance_bar_With_IDs.png')
                    except Exception as e:
                        print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ With_IDs R2 åœ–: {e}")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

