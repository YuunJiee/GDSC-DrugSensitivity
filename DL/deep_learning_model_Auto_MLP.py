import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import StandardScaler  # â­ ç”¨æ–¼ç‰¹å¾µæ¨™æº–åŒ–
from scipy.stats import spearmanr
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers, callbacks
from tensorflow.keras.models import Model
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šç¹ªåœ–é¢¨æ ¼
plt.style.use('ggplot')
sns.set_palette("husl")


def check_gpu_availability():
    """
    æª¢æŸ¥ä¸¦é¡¯ç¤ºå¯ç”¨çš„ç¡¬é«”è¨­å‚™ï¼ˆCPU/GPUï¼‰
    
    å°æ–¼ Apple Silicon (M1/M2/M3)ï¼ŒTensorFlow ä½¿ç”¨ Metal å¾Œç«¯é€²è¡Œ GPU åŠ é€Ÿ
    
    è¿”å›:
        device_info: åŒ…å«è¨­å‚™è³‡è¨Šçš„å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ” ç¡¬é«”è¨­å‚™æª¢æ¸¬")
    print("="*60)
    
    # TensorFlow ç‰ˆæœ¬
    print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
    
    # æª¢æŸ¥å¯ç”¨çš„ç‰©ç†è¨­å‚™
    physical_devices = tf.config.list_physical_devices()
    print(f"\næ‰€æœ‰å¯ç”¨è¨­å‚™:")
    for device in physical_devices:
        print(f"  - {device.device_type}: {device.name}")
    
    # æª¢æŸ¥ GPU
    gpus = tf.config.list_physical_devices('GPU')
    gpu_available = len(gpus) > 0
    
    print(f"\nğŸ–¥ï¸  GPU è¨­å‚™:")
    if gpu_available:
        print(f"  âœ… æ‰¾åˆ° {len(gpus)} å€‹ GPU")
        for i, gpu in enumerate(gpus):
            print(f"     [{i}] {gpu.name}")
        
        # å°æ–¼ Apple Siliconï¼Œé¡¯ç¤ºé¡å¤–è³‡è¨Š
        import platform
        if platform.processor() == 'arm':
            print(f"\n  ğŸ Apple Silicon åµæ¸¬:")
            print(f"     è™•ç†å™¨: {platform.processor()}")
            print(f"     ç³»çµ±: {platform.system()} {platform.release()}")
            print(f"     âœ… ä½¿ç”¨ Metal å¾Œç«¯é€²è¡Œ GPU åŠ é€Ÿ")
    else:
        print(f"  âš ï¸  æœªæ‰¾åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")
    
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨æ··åˆç²¾åº¦
    try:
        from tensorflow.keras import mixed_precision
        policy = mixed_precision.global_policy()
        print(f"\nâš¡ æ··åˆç²¾åº¦ç­–ç•¥: {policy.name}")
    except:
        pass
    
    # å»ºè­°è¨­å®š
    print(f"\nğŸ’¡ è¨“ç·´åŠ é€Ÿå»ºè­°:")
    if gpu_available:
        print(f"  âœ… GPU å·²å•Ÿç”¨ï¼Œè¨“ç·´é€Ÿåº¦å°‡å¤§å¹…æå‡")
        print(f"  ğŸ’¡ å¦‚éœ€é€²ä¸€æ­¥åŠ é€Ÿï¼Œå¯å˜—è©¦:")
        print(f"     - å¢åŠ  batch_sizeï¼ˆå¦‚æœè¨˜æ†¶é«”è¶³å¤ ï¼‰")
        print(f"     - å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ï¼ˆfloat16ï¼‰")
    else:
        print(f"  ğŸ’¡ è‹¥è¦ä½¿ç”¨ GPU åŠ é€Ÿ:")
        print(f"     - ç¢ºèªå·²å®‰è£ tensorflow-metalï¼ˆApple Siliconï¼‰")
        print(f"     - å®‰è£æŒ‡ä»¤: pip install tensorflow-metal")
    
    print("="*60 + "\n")
    
    device_info = {
        'gpu_available': gpu_available,
        'num_gpus': len(gpus),
        'gpu_names': [gpu.name for gpu in gpus],
        'tf_version': tf.__version__
    }
    
    return device_info



def build_autoencoder(input_dim, encoding_dim=128, l2_reg=0.001):
    """
    å»ºç«‹ Autoencoder ç”¨æ–¼ç‰¹å¾µé™ç¶­
    
    æ¶æ§‹: input -> 512 -> 256 -> encoding_dim -> 256 -> 512 -> output
    
    åƒæ•¸:
        input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
        encoding_dim: ç·¨ç¢¼å±¤ç¶­åº¦ï¼ˆå£“ç¸®å¾Œçš„ç‰¹å¾µæ•¸ï¼‰
        l2_reg: L2 æ­£å‰‡åŒ–å¼·åº¦
    
    è¿”å›:
        encoder: ç·¨ç¢¼å™¨æ¨¡å‹
        autoencoder: å®Œæ•´çš„è‡ªç·¨ç¢¼å™¨æ¨¡å‹
    """
    # è¼¸å…¥å±¤
    input_layer = layers.Input(shape=(input_dim,))
    
    # ç·¨ç¢¼å™¨
    encoded = layers.Dense(512, activation='relu', 
                          kernel_regularizer=regularizers.l2(l2_reg))(input_layer)
    encoded = layers.BatchNormalization()(encoded)
    encoded = layers.Dropout(0.3)(encoded)
    
    encoded = layers.Dense(256, activation='relu',
                          kernel_regularizer=regularizers.l2(l2_reg))(encoded)
    encoded = layers.BatchNormalization()(encoded)
    encoded = layers.Dropout(0.2)(encoded)
    
    # å£“ç¸®å±¤ï¼ˆç“¶é ¸å±¤ï¼‰
    encoded = layers.Dense(encoding_dim, activation='relu',
                          kernel_regularizer=regularizers.l2(l2_reg),
                          name='encoded_features')(encoded)
    
    # è§£ç¢¼å™¨
    decoded = layers.Dense(256, activation='relu',
                          kernel_regularizer=regularizers.l2(l2_reg))(encoded)
    decoded = layers.BatchNormalization()(decoded)
    decoded = layers.Dropout(0.2)(decoded)
    
    decoded = layers.Dense(512, activation='relu',
                          kernel_regularizer=regularizers.l2(l2_reg))(decoded)
    decoded = layers.BatchNormalization()(decoded)
    
    # è¼¸å‡ºå±¤
    decoded = layers.Dense(input_dim, activation='linear')(decoded)
    
    # å»ºç«‹æ¨¡å‹
    autoencoder = Model(inputs=input_layer, outputs=decoded, name='autoencoder')
    encoder = Model(inputs=input_layer, outputs=encoded, name='encoder')
    
    return encoder, autoencoder


def build_mlp_model(input_dim, l2_reg=0.0001, dropout_rate=0.3):  # â­ é™ä½ L2 æ­£å‰‡åŒ–
    """
    å»ºç«‹ MLP å›æ­¸æ¨¡å‹ï¼ˆå¢å¼·ç‰ˆï¼‰
    
    æ¶æ§‹: input -> 256 -> 128 -> 64 -> 32 -> 16 -> 1
    
    åƒæ•¸:
        input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦ï¼ˆç·¨ç¢¼å¾Œçš„ç‰¹å¾µæ•¸ï¼‰
        l2_reg: L2 æ­£å‰‡åŒ–å¼·åº¦
        dropout_rate: Dropout æ¯”ç‡
    
    è¿”å›:
        model: MLP æ¨¡å‹
    """
    model = keras.Sequential([
        layers.Input(shape=(input_dim,)),
        
        # â­ æ–°å¢ï¼šç¬¬ä¸€å±¤å¢åŠ ç¥ç¶“å…ƒæ•¸é‡
        layers.Dense(256, activation='relu', 
                    kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate),
        
        # â­ æ–°å¢ï¼šç¬¬äºŒå±¤
        layers.Dense(128, activation='relu',
                    kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate * 0.8),
        
        layers.Dense(64, activation='relu', 
                    kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate * 0.6),
        
        layers.Dense(32, activation='relu',
                    kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate * 0.4),
        
        layers.Dense(16, activation='relu',
                    kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        
        layers.Dense(1, activation='linear')  # å›æ­¸è¼¸å‡º
    ], name='mlp_regressor')
    
    return model


def train_autoencoder(X_train, X_val, epochs=100, batch_size=128, verbose=1):
    """
    è¨“ç·´ Autoencoder
    
    åƒæ•¸:
        X_train: è¨“ç·´è³‡æ–™
        X_val: é©—è­‰è³‡æ–™
        epochs: è¨“ç·´è¼ªæ•¸
        batch_size: æ‰¹æ¬¡å¤§å°
        verbose: é¡¯ç¤ºè©³ç´°ç¨‹åº¦
    
    è¿”å›:
        encoder: è¨“ç·´å¥½çš„ç·¨ç¢¼å™¨
        autoencoder: è¨“ç·´å¥½çš„è‡ªç·¨ç¢¼å™¨
        history: è¨“ç·´æ­·å²
    """
    print("\n" + "="*50)
    print("éšæ®µ 1: è¨“ç·´ Autoencoder é€²è¡Œç‰¹å¾µé™ç¶­")
    print("="*50)
    
    input_dim = X_train.shape[1]
    encoding_dim = 256  # â­ å¾ 128 æå‡åˆ° 256ï¼Œä¿ç•™æ›´å¤šç‰¹å¾µè³‡è¨Š
    
    # å»ºç«‹æ¨¡å‹
    encoder, autoencoder = build_autoencoder(input_dim, encoding_dim)
    
    # ç·¨è­¯æ¨¡å‹
    # ä½¿ç”¨ legacy Adam optimizer ä»¥å…¼å®¹ Apple Silicon (M1/M2/M3)
    autoencoder.compile(
        # optimizer=keras.optimizers.Adam(learning_rate=0.001), # ç‚ºä½¿ç”¨
        optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005),  # â­ é™ä½å­¸ç¿’ç‡æå‡ç©©å®šæ€§
        loss='mse',
        metrics=['mae']
    )
    
    # Callbacks
    early_stop = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=1e-6,
        verbose=1
    )
    
    # è¨“ç·´
    history = autoencoder.fit(
        X_train, X_train,
        validation_data=(X_val, X_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop, reduce_lr],
        verbose=verbose
    )
    
    print(f"\nâœ“ Autoencoder è¨“ç·´å®Œæˆ")
    print(f"  - åŸå§‹ç‰¹å¾µæ•¸: {input_dim}")
    print(f"  - å£“ç¸®ç‰¹å¾µæ•¸: {encoding_dim}")
    print(f"  - å£“ç¸®æ¯”ç‡: {encoding_dim/input_dim*100:.1f}%")
    
    return encoder, autoencoder, history


def train_mlp_model(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, verbose=1):
    """
    è¨“ç·´ MLP å›æ­¸æ¨¡å‹
    
    åƒæ•¸:
        X_train: è¨“ç·´ç‰¹å¾µï¼ˆç·¨ç¢¼å¾Œï¼‰
        y_train: è¨“ç·´æ¨™ç±¤
        X_val: é©—è­‰ç‰¹å¾µï¼ˆç·¨ç¢¼å¾Œï¼‰
        y_val: é©—è­‰æ¨™ç±¤
        epochs: è¨“ç·´è¼ªæ•¸
        batch_size: æ‰¹æ¬¡å¤§å°
        verbose: é¡¯ç¤ºè©³ç´°ç¨‹åº¦
    
    è¿”å›:
        model: è¨“ç·´å¥½çš„ MLP æ¨¡å‹
        history: è¨“ç·´æ­·å²
    """
    print("\n" + "="*50)
    print("éšæ®µ 2: è¨“ç·´ MLP å›æ­¸æ¨¡å‹")
    print("="*50)
    
    input_dim = X_train.shape[1]
    
    # å»ºç«‹æ¨¡å‹
    model = build_mlp_model(input_dim)
    
    # ç·¨è­¯æ¨¡å‹
    # ä½¿ç”¨ legacy Adam optimizer ä»¥å…¼å®¹ Apple Silicon (M1/M2/M3)
    model.compile(
        optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005),  # â­ é™ä½å­¸ç¿’ç‡
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    # Callbacks
    early_stop = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=25,
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-7,
        verbose=1
    )
    
    # è¨“ç·´
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop, reduce_lr],
        verbose=verbose
    )
    
    print(f"\nâœ“ MLP æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    return model, history


def evaluate_dl_model(y_true, y_pred, model_name="Deep Learning"):
    """
    è©•ä¼°æ·±åº¦å­¸ç¿’æ¨¡å‹
    
    è¨ˆç®—å¤šç¨®è©•ä¼°æŒ‡æ¨™: MAE, MSE, RMSE, RÂ², Spearman Correlation
    
    åƒæ•¸:
        y_true: çœŸå¯¦å€¼
        y_pred: é æ¸¬å€¼
        model_name: æ¨¡å‹åç¨±
    
    è¿”å›:
        metrics: åŒ…å«æ‰€æœ‰æŒ‡æ¨™çš„å­—å…¸
    """
    # å±•å¹³é æ¸¬å€¼ï¼ˆç¢ºä¿æ˜¯ä¸€ç¶­ï¼‰
    y_pred = y_pred.flatten()
    
    # è¨ˆç®—æŒ‡æ¨™
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    
    # Spearman ç›¸é—œä¿‚æ•¸
    spearman_corr, spearman_pval = spearmanr(y_true, y_pred)
    
    metrics = {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'R2': r2,
        'Spearman_Correlation': spearman_corr,
        'Spearman_PValue': spearman_pval
    }
    
    # è¼¸å‡ºçµæœ
    print("\n" + "="*50)
    print(f"{model_name} æ¨¡å‹è©•ä¼°çµæœ")
    print("="*50)
    print(f"  MAE  (Mean Absolute Error):     {mae:.4f}")
    print(f"  MSE  (Mean Squared Error):      {mse:.4f}")
    print(f"  RMSE (Root Mean Squared Error): {rmse:.4f}")
    print(f"  RÂ²   (R-squared):               {r2:.4f}")
    print(f"  Spearman Correlation:           {spearman_corr:.4f} (p={spearman_pval:.4e})")
    print("="*50)
    
    return metrics


def calculate_feature_importance(model, encoder, X_test, y_test, 
                                 feature_names, n_repeats=10, random_state=42):
    """
    ä½¿ç”¨ Permutation Importance è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
    
    åƒæ•¸:
        model: MLP æ¨¡å‹
        encoder: Autoencoder ç·¨ç¢¼å™¨
        X_test: æ¸¬è©¦è³‡æ–™ï¼ˆåŸå§‹ç‰¹å¾µï¼‰- numpy array æˆ– DataFrame
        y_test: æ¸¬è©¦æ¨™ç±¤ - numpy array æˆ– Series
        feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨æˆ– Index
        n_repeats: æ’åˆ—é‡è¤‡æ¬¡æ•¸
        random_state: éš¨æ©Ÿç¨®å­
    
    è¿”å›:
        importance_df: ç‰¹å¾µé‡è¦æ€§ DataFrame
    """
    print("\nè¨ˆç®—ç‰¹å¾µé‡è¦æ€§ï¼ˆPermutation Importanceï¼‰...")
    
    # ç¢ºä¿ X_test æ˜¯ numpy array
    if hasattr(X_test, 'values'):
        X_test = X_test.values
    X_test = np.asarray(X_test, dtype=np.float32)
    
    # ç¢ºä¿ y_test æ˜¯ numpy array
    if hasattr(y_test, 'values'):
        y_test = y_test.values
    y_test = np.asarray(y_test, dtype=np.float32).flatten()
    
    # ç¢ºä¿ feature_names æ˜¯åˆ—è¡¨
    if hasattr(feature_names, 'tolist'):
        feature_names = feature_names.tolist()
    feature_names = list(feature_names)
    
    # æª¢æŸ¥ç¶­åº¦åŒ¹é…
    if X_test.shape[1] != len(feature_names):
        raise ValueError(
            f"ç‰¹å¾µæ•¸é‡ä¸åŒ¹é…: X_test æœ‰ {X_test.shape[1]} å€‹ç‰¹å¾µï¼Œ"
            f"ä½† feature_names æœ‰ {len(feature_names)} å€‹åç¨±"
        )
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    np.random.seed(random_state)
    
    # å®šç¾©å®Œæ•´çš„é æ¸¬æµç¨‹
    def predict_pipeline(X):
        X_encoded = encoder.predict(X, verbose=0)
        return model.predict(X_encoded, verbose=0).flatten()
    
    # è¨ˆç®—åŸºæº–åˆ†æ•¸
    y_pred_baseline = predict_pipeline(X_test)
    baseline_score = r2_score(y_test, y_pred_baseline)
    
    print(f"  åŸºæº– RÂ² åˆ†æ•¸: {baseline_score:.4f}")
    print(f"  è¨ˆç®— {len(feature_names)} å€‹ç‰¹å¾µçš„é‡è¦æ€§...")
    
    # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„é‡è¦æ€§
    importances = []
    for i, feature_name in enumerate(feature_names):
        if (i + 1) % 50 == 0:  # æ¯ 50 å€‹ç‰¹å¾µé¡¯ç¤ºé€²åº¦
            print(f"    é€²åº¦: {i+1}/{len(feature_names)}")
        
        scores = []
        for _ in range(n_repeats):
            # è¤‡è£½æ¸¬è©¦è³‡æ–™ - ç¢ºä¿æ˜¯ numpy array
            X_permuted = np.copy(X_test)  # ä½¿ç”¨ np.copy è€Œä¸æ˜¯ .copy()
            # éš¨æ©Ÿæ’åˆ—è©²ç‰¹å¾µ
            X_permuted[:, i] = np.random.permutation(X_permuted[:, i])
            # é æ¸¬
            y_pred_permuted = predict_pipeline(X_permuted)
            # è¨ˆç®—åˆ†æ•¸ä¸‹é™
            score = r2_score(y_test, y_pred_permuted)
            scores.append(baseline_score - score)
        
        importances.append({
            'feature': feature_name,
            'importance': np.mean(scores),
            'std': np.std(scores)
        })
    
    # è½‰æ›ç‚º DataFrame ä¸¦æ’åº
    importance_df = pd.DataFrame(importances)
    importance_df = importance_df.sort_values('importance', ascending=False)
    
    print(f"âœ“ ç‰¹å¾µé‡è¦æ€§è¨ˆç®—å®Œæˆ")
    
    return importance_df



def plot_learning_curves(ae_history, mlp_history, save_path='dl_learning_curves.png'):
    """
    ç¹ªè£½å­¸ç¿’æ›²ç·š
    
    åƒæ•¸:
        ae_history: Autoencoder è¨“ç·´æ­·å²
        mlp_history: MLP è¨“ç·´æ­·å²
        save_path: å„²å­˜è·¯å¾‘
    """
    fig, axes = plt.subplots(1, 2, figsize=(16, 5))
    
    # Autoencoder å­¸ç¿’æ›²ç·š
    ax1 = axes[0]
    ax1.plot(ae_history.history['loss'], label='Training Loss', linewidth=2)
    ax1.plot(ae_history.history['val_loss'], label='Validation Loss', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss (MSE)', fontsize=12)
    ax1.set_title('Autoencoder Learning Curves', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # MLP å­¸ç¿’æ›²ç·š
    ax2 = axes[1]
    ax2.plot(mlp_history.history['loss'], label='Training Loss', linewidth=2)
    ax2.plot(mlp_history.history['val_loss'], label='Validation Loss', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Loss (MSE)', fontsize=12)
    ax2.set_title('MLP Regressor Learning Curves', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ å­¸ç¿’æ›²ç·šå·²å„²å­˜: {save_path}")
    plt.close()


def plot_predictions_vs_actual(y_test, y_pred, metrics, save_path='dl_predictions_vs_actual.png'):
    """
    ç¹ªè£½é æ¸¬å€¼ vs å¯¦éš›å€¼æ•£é»åœ–
    
    åƒæ•¸:
        y_test: çœŸå¯¦å€¼
        y_pred: é æ¸¬å€¼
        metrics: è©•ä¼°æŒ‡æ¨™å­—å…¸
        save_path: å„²å­˜è·¯å¾‘
    """
    y_pred = y_pred.flatten()
    
    plt.figure(figsize=(10, 8))
    
    # æ•£é»åœ–
    plt.scatter(y_test, y_pred, alpha=0.5, s=30, edgecolors='k', linewidths=0.5)
    
    # å°è§’ç·šï¼ˆå®Œç¾é æ¸¬ï¼‰
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
    
    # æ¨™é¡Œå’Œæ¨™ç±¤
    plt.xlabel('Actual LN_IC50', fontsize=13, fontweight='bold')
    plt.ylabel('Predicted LN_IC50', fontsize=13, fontweight='bold')
    plt.title('Deep Learning Model: Predictions vs Actual', fontsize=15, fontweight='bold')
    
    # æ·»åŠ è©•ä¼°æŒ‡æ¨™æ–‡å­—
    textstr = f"RÂ² = {metrics['R2']:.4f}\n"
    textstr += f"RMSE = {metrics['RMSE']:.4f}\n"
    textstr += f"MAE = {metrics['MAE']:.4f}\n"
    textstr += f"Spearman Ï = {metrics['Spearman_Correlation']:.4f}"
    
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=11,
            verticalalignment='top', bbox=props)
    
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ é æ¸¬åœ–å·²å„²å­˜: {save_path}")
    plt.close()


def plot_feature_importance(importance_df, top_n=20, save_path='dl_feature_importance.png'):
    """
    ç¹ªè£½ç‰¹å¾µé‡è¦æ€§åœ–
    
    åƒæ•¸:
        importance_df: ç‰¹å¾µé‡è¦æ€§ DataFrame
        top_n: é¡¯ç¤ºå‰ N å€‹é‡è¦ç‰¹å¾µ
        save_path: å„²å­˜è·¯å¾‘
    """
    top_features = importance_df.head(top_n)
    
    plt.figure(figsize=(12, 8))
    
    # æ©«å‘æ¢å½¢åœ–
    y_pos = np.arange(len(top_features))
    plt.barh(y_pos, top_features['importance'], xerr=top_features['std'],
            alpha=0.8, edgecolor='black', linewidth=1.2)
    
    plt.yticks(y_pos, top_features['feature'], fontsize=10)
    plt.xlabel('Importance (RÂ² decrease)', fontsize=12, fontweight='bold')
    plt.title(f'Top {top_n} Feature Importance (Permutation)', fontsize=14, fontweight='bold')
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜: {save_path}")
    plt.close()


def run_deep_learning_pipeline(X_train, X_test, y_train, y_test, feature_names):
    """
    åŸ·è¡Œå®Œæ•´çš„æ·±åº¦å­¸ç¿’ Pipeline
    
    æµç¨‹:
        1. æª¢æ¸¬ç¡¬é«”è¨­å‚™ï¼ˆGPU/CPUï¼‰
        2. è³‡æ–™åˆ†å‰²ï¼ˆè¨“ç·´/é©—è­‰ï¼‰
        3. è¨“ç·´ Autoencoder é™ç¶­
        4. ä½¿ç”¨ Encoder è½‰æ›ç‰¹å¾µ
        5. è¨“ç·´ MLP å›æ­¸æ¨¡å‹
        6. è©•ä¼°æ¨¡å‹
        7. ç¹ªè£½è¦–è¦ºåŒ–åœ–è¡¨
    
    åƒæ•¸:
        X_train: è¨“ç·´ç‰¹å¾µ
        X_test: æ¸¬è©¦ç‰¹å¾µ
        y_train: è¨“ç·´æ¨™ç±¤
        y_test: æ¸¬è©¦æ¨™ç±¤
        feature_names: ç‰¹å¾µåç¨±
    
    è¿”å›:
        encoder: è¨“ç·´å¥½çš„ç·¨ç¢¼å™¨
        mlp_model: è¨“ç·´å¥½çš„ MLP æ¨¡å‹
        y_pred: æ¸¬è©¦é›†é æ¸¬å€¼
        metrics: è©•ä¼°æŒ‡æ¨™
    """
    print("\n" + "ğŸš€ "*20)
    print("é–‹å§‹æ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´ Pipeline")
    print("ğŸš€ "*20)
    
    # Step 0: æª¢æ¸¬ç¡¬é«”è¨­å‚™
    device_info = check_gpu_availability()
    
    # â­ Step 0.5: ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆç¢ºä¿æ•¸æ“šå·²æ¨™æº–åŒ–ï¼‰
    print("\n" + "="*60)
    print("ç‰¹å¾µæ¨™æº–åŒ–æª¢æŸ¥èˆ‡è™•ç†")
    print("="*60)
    
    # è½‰æ›ç‚º numpy arrayï¼ˆå¦‚æœæ˜¯ DataFrameï¼‰
    if hasattr(X_train, 'values'):
        X_train_np = X_train.values
        X_test_np = X_test.values
        feature_cols = X_train.columns if hasattr(X_train, 'columns') else feature_names
    else:
        X_train_np = np.asarray(X_train)
        X_test_np = np.asarray(X_test)
        feature_cols = feature_names
    
    # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ¨™æº–åŒ–ï¼ˆæª¢æŸ¥å‡å€¼å’Œæ¨™æº–å·®ï¼‰
    train_mean = np.mean(X_train_np)
    train_std = np.std(X_train_np)
    
    print(f"  åŸå§‹ç‰¹å¾µçµ±è¨ˆ:")
    print(f"    å‡å€¼: {train_mean:.4f}")
    print(f"    æ¨™æº–å·®: {train_std:.4f}")
    print(f"    ç¯„åœ: [{np.min(X_train_np):.2f}, {np.max(X_train_np):.2f}]")
    
    # å¦‚æœæ•¸æ“šçœ‹èµ·ä¾†æœªæ¨™æº–åŒ–ï¼ˆå‡å€¼ä¸æ¥è¿‘0æˆ–æ¨™æº–å·®ä¸æ¥è¿‘1ï¼‰ï¼Œå‰‡é€²è¡Œæ¨™æº–åŒ–
    if abs(train_mean) > 0.1 or abs(train_std - 1.0) > 0.2:
        print(f"\n  âš ï¸  æª¢æ¸¬åˆ°ç‰¹å¾µæœªæ¨™æº–åŒ–ï¼Œæ­£åœ¨é€²è¡Œæ¨™æº–åŒ–...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_np)
        X_test_scaled = scaler.transform(X_test_np)
        
        print(f"  âœ… æ¨™æº–åŒ–å®Œæˆï¼")
        print(f"    æ–°å‡å€¼: {np.mean(X_train_scaled):.4f}")
        print(f"    æ–°æ¨™æº–å·®: {np.std(X_train_scaled):.4f}")
        print(f"    æ–°ç¯„åœ: [{np.min(X_train_scaled):.2f}, {np.max(X_train_scaled):.2f}]")
        
        # ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š
        X_train_np = X_train_scaled
        X_test_np = X_test_scaled
    else:
        print(f"  âœ… ç‰¹å¾µå·²æ¨™æº–åŒ–ï¼Œè·³éæ¨™æº–åŒ–æ­¥é©Ÿ")
    
    print("="*60)
    
    # å¾è¨“ç·´é›†ä¸­åˆ†å‡ºé©—è­‰é›†
    X_train_split, X_val, y_train_split, y_val = train_test_split(
        X_train_np, y_train, test_size=0.15, random_state=42
    )
    
    print(f"\nè³‡æ–™é›†å¤§å°:")
    print(f"  è¨“ç·´é›†: {X_train_split.shape}")
    print(f"  é©—è­‰é›†: {X_val.shape}")
    print(f"  æ¸¬è©¦é›†: {X_test.shape}")
    
    # Step 1: è¨“ç·´ Autoencoder
    encoder, autoencoder, ae_history = train_autoencoder(
        X_train_split, X_val, 
        epochs=70, 
        batch_size=128,
        verbose=1
    )
    
    # Step 2: ä½¿ç”¨ Encoder è½‰æ›è³‡æ–™
    print("\nè½‰æ›è³‡æ–™è‡³ä½ç¶­ç©ºé–“...")
    X_train_encoded = encoder.predict(X_train_split, verbose=0)
    X_val_encoded = encoder.predict(X_val, verbose=0)
    X_test_encoded = encoder.predict(X_test_np, verbose=0)  # â­ ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š
    print(f"âœ“ ç‰¹å¾µç¶­åº¦: {X_train_np.shape[1]} â†’ {X_train_encoded.shape[1]}")
    
    # Step 3: è¨“ç·´ MLP æ¨¡å‹
    mlp_model, mlp_history = train_mlp_model(
        X_train_encoded, y_train_split,
        X_val_encoded, y_val,
        epochs=200,
        batch_size=64,
        verbose=1
    )
    
    # Step 4: é æ¸¬
    print("\né€²è¡Œé æ¸¬...")
    y_pred = mlp_model.predict(X_test_encoded, verbose=0)
    
    # Step 5: è©•ä¼°
    metrics = evaluate_dl_model(y_test, y_pred, model_name="Deep Learning (Autoencoder + MLP)")
    
    # Step 6: è¦–è¦ºåŒ–
    print("\n" + "="*50)
    print("ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨")
    print("="*50)
    
    plot_learning_curves(ae_history, mlp_history)
    plot_predictions_vs_actual(y_test, y_pred, metrics)
    
    # # Step 7: ç‰¹å¾µé‡è¦æ€§ï¼ˆå¯é¸ï¼Œè¼ƒè€—æ™‚ï¼‰
    # print("\næ˜¯å¦è¨ˆç®—ç‰¹å¾µé‡è¦æ€§? ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰")
    # importance_df = calculate_feature_importance(
    #     mlp_model, encoder, X_test_np, y_test,  # â­ ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š
    #     feature_names, n_repeats=5
    # )
    # plot_feature_importance(importance_df, top_n=20)
    
    print("\n" + "âœ… "*20)
    print("æ·±åº¦å­¸ç¿’ Pipeline åŸ·è¡Œå®Œæˆï¼")
    print("âœ… "*20 + "\n")
    
    return encoder, mlp_model, y_pred, metrics


# ==================== ä¸»ç¨‹å¼åŸ·è¡Œå€ ====================
if __name__ == "__main__":
    """
    ç¨ç«‹åŸ·è¡Œæ­¤æª”æ¡ˆé€²è¡Œæ¸¬è©¦
    """
    print("æ·±åº¦å­¸ç¿’æ¨¡å‹ - ç¨ç«‹åŸ·è¡Œæ¨¡å¼")
    print("="*60)
    
    # è¼‰å…¥èˆ‡å‰è™•ç†è³‡æ–™
    from main import preprocess_data
    
    file_path = 'Preprocessing/Data_imputed.csv'
    
    try:
        # è³‡æ–™è™•ç†
        X_train, X_test, y_train, y_test, features = preprocess_data(file_path)
        
        # åŸ·è¡Œæ·±åº¦å­¸ç¿’ Pipeline
        encoder, mlp_model, y_pred, metrics = run_deep_learning_pipeline(
            X_train, X_test, y_train, y_test, features
        )
        
        print("\næ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°å®Œæˆï¼")
        print("ç”Ÿæˆçš„æª”æ¡ˆ:")
        print("  - dl_learning_curves.png")
        print("  - dl_predictions_vs_actual.png")
        print("  - dl_feature_importance.png")
        
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

# result
# ==================================================
# Deep Learning (Autoencoder + MLP) æ¨¡å‹è©•ä¼°çµæœ
# ==================================================
#   MAE  (Mean Absolute Error):     1.8520
#   MSE  (Mean Squared Error):      5.6691
#   RMSE (Root Mean Squared Error): 2.3810
#   RÂ²   (R-squared):               0.2569
#   Spearman Correlation:           0.4237 (p=0.0000e+00)
# ==================================================