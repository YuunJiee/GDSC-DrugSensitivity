# æ·±åº¦å­¸ç¿’æ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¨¡å‹æ¶æ§‹

### ğŸ”¹ Autoencoderï¼ˆç‰¹å¾µé™ç¶­ï¼‰
```
è¼¸å…¥å±¤ (n å€‹ç‰¹å¾µ)
    â†“
Dense(512) + BatchNorm + Dropout(0.3)
    â†“
Dense(256) + BatchNorm + Dropout(0.2)
    â†“
Dense(128) - ç·¨ç¢¼å±¤ï¼ˆå£“ç¸®ç‰¹å¾µï¼‰
    â†“
Dense(256) + BatchNorm + Dropout(0.2)
    â†“
Dense(512) + BatchNorm
    â†“
è¼¸å‡ºå±¤ (n å€‹ç‰¹å¾µ) - é‡å»ºåŸå§‹è¼¸å…¥
```

### ğŸ”¹ MLP å›æ­¸æ¨¡å‹
```
è¼¸å…¥å±¤ (128 å€‹ç·¨ç¢¼ç‰¹å¾µ)
    â†“
Dense(64) + BatchNorm + Dropout(0.3)
    â†“
Dense(32) + BatchNorm + Dropout(0.24)
    â†“
Dense(16) + BatchNorm
    â†“
è¼¸å‡ºå±¤ (1) - é æ¸¬ LN_IC50
```

## ğŸš€ ä½¿ç”¨æ–¹å¼

### æ–¹æ³• 1ï¼šåŸ·è¡Œå®Œæ•´å°ˆæ¡ˆï¼ˆæ¨è–¦ï¼‰
```bash
# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source venv/bin/activate

# åŸ·è¡Œä¸»ç¨‹å¼ï¼ˆåŒ…å«åŸºç·šæ¨¡å‹ + æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼‰
python main.py
```

### æ–¹æ³• 2ï¼šåªåŸ·è¡Œæ·±åº¦å­¸ç¿’æ¨¡å‹
```bash
# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source venv/bin/activate

# å–®ç¨åŸ·è¡Œæ·±åº¦å­¸ç¿’æ¨¡å‹
python deep_learning_model.py
```

### æ–¹æ³• 3ï¼šåªåŸ·è¡ŒåŸºç·šæ¨¡å‹
åœ¨ `main.py` ä¸­è¨­å®šï¼š
```python
RUN_DEEP_LEARNING = False  # è¨­ç‚º False
```
ç„¶å¾ŒåŸ·è¡Œï¼š
```bash
python main.py
```

## ğŸ“Š è©•ä¼°æŒ‡æ¨™

æ·±åº¦å­¸ç¿’æ¨¡å‹æä¾›ä»¥ä¸‹è©•ä¼°æŒ‡æ¨™ï¼š

| æŒ‡æ¨™ | èªªæ˜ |
|------|------|
| **MAE** | Mean Absolute Errorï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼‰ |
| **MSE** | Mean Squared Errorï¼ˆå‡æ–¹èª¤å·®ï¼‰ |
| **RMSE** | Root Mean Squared Errorï¼ˆå‡æ–¹æ ¹èª¤å·®ï¼‰ |
| **RÂ²** | R-squaredï¼ˆæ±ºå®šä¿‚æ•¸ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ |
| **Spearman Ï** | Spearman Correlationï¼ˆç­‰ç´šç›¸é—œä¿‚æ•¸ï¼‰ |

## ğŸ“ˆ ç”Ÿæˆçš„è¦–è¦ºåŒ–åœ–è¡¨

åŸ·è¡Œå¾Œæœƒç”Ÿæˆä»¥ä¸‹åœ–è¡¨ï¼š

### åŸºç·šæ¨¡å‹ï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰
- `model_comparison.png` - RF å’Œ XGBoost çš„é æ¸¬æ¯”è¼ƒ

### æ·±åº¦å­¸ç¿’æ¨¡å‹
- `dl_learning_curves.png` - è¨“ç·´éç¨‹çš„å­¸ç¿’æ›²ç·š
- `dl_predictions_vs_actual.png` - é æ¸¬å€¼ vs å¯¦éš›å€¼æ•£é»åœ–
- `dl_feature_importance.png` - å‰ 20 å€‹é‡è¦ç‰¹å¾µ

## âš™ï¸ æ¨¡å‹é…ç½®

### Autoencoder åƒæ•¸
- **Encoding dimension**: 128ï¼ˆå¯èª¿æ•´ï¼‰
- **L2 regularization**: 0.001
- **Dropout**: 0.2-0.3
- **Optimizer**: Adam (lr=0.001)
- **Early stopping patience**: 15 epochs
- **Learning rate decay**: ReduceLROnPlateau

### MLP åƒæ•¸
- **Hidden layers**: [64, 32, 16]
- **L2 regularization**: 0.001
- **Dropout**: 0.24-0.3
- **Optimizer**: Adam (lr=0.001)
- **Early stopping patience**: 25 epochs
- **Learning rate decay**: ReduceLROnPlateau

## ğŸ”§ è‡ªå®šç¾©æ¨¡å‹

å¦‚æœæƒ³èª¿æ•´æ¨¡å‹æ¶æ§‹ï¼Œå¯ä»¥ä¿®æ”¹ `deep_learning_model.py` ä¸­çš„å‡½æ•¸ï¼š

```python
# ä¿®æ”¹ç·¨ç¢¼ç¶­åº¦
def build_autoencoder(input_dim, encoding_dim=64, ...):  # åŸç‚º 128

# ä¿®æ”¹ MLP å±¤æ•¸
def build_mlp_model(input_dim, ...):
    model = keras.Sequential([
        layers.Dense(128, ...),  # å¯å¢åŠ ç¥ç¶“å…ƒæ•¸é‡
        layers.Dense(64, ...),
        # å¯æ·»åŠ æ›´å¤šå±¤
    ])
```

## ğŸ“ åŒ¯å‡ºæ¨¡å‹ä¾›å…¶ä»–ç”¨é€”

åœ¨ä½ çš„ç¨‹å¼ä¸­åŒ¯å…¥ï¼š

```python
from deep_learning_model import run_deep_learning_pipeline

# åŸ·è¡Œå®Œæ•´ pipeline
encoder, mlp_model, predictions, metrics = run_deep_learning_pipeline(
    X_train, X_test, y_train, y_test, feature_names
)

# ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬
X_encoded = encoder.predict(X_new)
y_pred = mlp_model.predict(X_encoded)
```

## â±ï¸ é è¨ˆåŸ·è¡Œæ™‚é–“

- **Autoencoder è¨“ç·´**: 5-10 åˆ†é˜
- **MLP è¨“ç·´**: 3-8 åˆ†é˜
- **ç‰¹å¾µé‡è¦æ€§è¨ˆç®—**: 5-15 åˆ†é˜ï¼ˆå¯é¸ï¼‰
- **ç¸½è¨ˆ**: ~15-30 åˆ†é˜ï¼ˆå–æ±ºæ–¼ç¡¬é«”å’Œè³‡æ–™å¤§å°ï¼‰

## ğŸ’¡ æç¤º

1. **GPU åŠ é€Ÿ**: å¦‚æœæœ‰ GPUï¼ŒTensorFlow æœƒè‡ªå‹•ä½¿ç”¨ï¼Œè¨“ç·´é€Ÿåº¦å¯æå‡ 5-10 å€
2. **è¨˜æ†¶é«”éœ€æ±‚**: å»ºè­°è‡³å°‘ 8GB RAM
3. **Early Stopping**: æœƒè‡ªå‹•åœ¨é©—è­‰æå¤±ä¸å†æ”¹å–„æ™‚åœæ­¢è¨“ç·´
4. **ç‰¹å¾µé‡è¦æ€§**: è¨ˆç®—è¼ƒè€—æ™‚ï¼Œå¯åœ¨ `run_deep_learning_pipeline()` ä¸­è¨»è§£æ‰

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q: è¨“ç·´æ™‚é–“éé•·ï¼Ÿ
A: å¯ä»¥æ¸›å°‘ epochs æ•¸é‡æˆ– n_repeatsï¼ˆç‰¹å¾µé‡è¦æ€§ï¼‰

### Q: æ¨¡å‹æ•ˆæœä¸ä½³ï¼Ÿ
A: å˜—è©¦ï¼š
- èª¿æ•´ encoding_dimï¼ˆç‰¹å¾µå£“ç¸®ç¨‹åº¦ï¼‰
- å¢åŠ /æ¸›å°‘æ­£å‰‡åŒ–å¼·åº¦
- èª¿æ•´ Dropout æ¯”ç‡
- å¢åŠ è¨“ç·´è³‡æ–™é‡

### Q: è¨˜æ†¶é«”ä¸è¶³ï¼Ÿ
A: æ¸›å°‘ batch_size æˆ– encoding_dim
